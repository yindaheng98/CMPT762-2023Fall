{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "This is a template notebook for Assignment 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## Install dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-i4hmGYk1dL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some common libraries\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import some common pytorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tUA_j6AF1L5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that GPU is available for your notebook.\n",
    "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A_Di_fgL4HSv"
   },
   "outputs": [],
   "source": [
    "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
    "# TODO: approx 1 line\n",
    "BASE_DIR = '.'\n",
    "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## Part 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRV-KFJzlur4"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(os.path.join(data_dirs, \"train.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"validate\" else train_annotations\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCH-2mWxDVVu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"validate\", \"test\"]:\n",
    "    DatasetCatalog.register(i, lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(i).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qNSdXCL_DVAz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "plane_metadata = MetadataCatalog.get(\"train\")\n",
    "train_set = get_detection_data(\"train\")\n",
    "data = train_set[random.randrange(0, len(train_set))]\n",
    "img = cv2.imread(data[\"file_name\"])\n",
    "visualizer = Visualizer(img[:, :, ::-1], metadata=plane_metadata, scale=0.5)\n",
    "out = visualizer.draw_dataset_dict(data)\n",
    "save_path = os.path.join(BASE_DIR, \"output\", \"train_set.jpg\")\n",
    "cv2.imwrite(save_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4rql8pNokE4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7d3KxiHO_0gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:33:13 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:33:14 d2.data.build]: Removed 0 images with no usable annotations. 159 images left.\n",
      "[10/21 00:33:14 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 6384         |             |              |\n",
      "|    total    | 6384         |             |              |             |              |\n",
      "[10/21 00:33:14 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[10/21 00:33:14 d2.data.build]: Using training sampler TrainingSampler\n",
      "[10/21 00:33:14 d2.data.common]: Serializing 159 elements to byte tensors and concatenating them all ...\n",
      "[10/21 00:33:14 d2.data.common]: Serialized dataset takes 13.26 MiB\n",
      "WARNING [10/21 00:33:14 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R-101.pkl: 179MB [00:04, 42.6MB/s]                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:33:18 d2.checkpoint.c2_model_loading]: Renaming Caffe2 weights ......\n",
      "[10/21 00:33:19 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone.bottom_up:\n",
      "| Names in Model    | Names in Checkpoint       | Shapes                                          |\n",
      "|:------------------|:--------------------------|:------------------------------------------------|\n",
      "| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |\n",
      "| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |\n",
      "| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |\n",
      "| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |\n",
      "| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |\n",
      "| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |\n",
      "| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |\n",
      "| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |\n",
      "| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "backbone.fpn_lateral2.{bias, weight}\n",
      "backbone.fpn_lateral3.{bias, weight}\n",
      "backbone.fpn_lateral4.{bias, weight}\n",
      "backbone.fpn_lateral5.{bias, weight}\n",
      "backbone.fpn_output2.{bias, weight}\n",
      "backbone.fpn_output3.{bias, weight}\n",
      "backbone.fpn_output4.{bias, weight}\n",
      "backbone.fpn_output5.{bias, weight}\n",
      "proposal_generator.rpn_head.anchor_deltas.{bias, weight}\n",
      "proposal_generator.rpn_head.conv.{bias, weight}\n",
      "proposal_generator.rpn_head.objectness_logits.{bias, weight}\n",
      "roi_heads.box_head.fc1.{bias, weight}\n",
      "roi_heads.box_head.fc2.{bias, weight}\n",
      "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
      "roi_heads.box_predictor.cls_score.{bias, weight}\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  fc1000.{bias, weight}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:33:19 d2.engine.train_loop]: Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:33:51 d2.utils.events]:  eta: 0:02:24  iter: 19  total_loss: 5.684  loss_cls: 4.453  loss_box_reg: 0.02164  loss_rpn_cls: 0.72  loss_rpn_loc: 0.6866  time: 1.1810  data_time: 1.1692  lr: 9.7405e-06  max_mem: 4859M\n",
      "[10/21 00:34:19 d2.utils.events]:  eta: 0:02:19  iter: 39  total_loss: 2.316  loss_cls: 0.877  loss_box_reg: 0.01582  loss_rpn_cls: 0.7083  loss_rpn_loc: 0.5667  time: 1.2926  data_time: 1.0834  lr: 1.9731e-05  max_mem: 4859M\n",
      "[10/21 00:34:36 d2.utils.events]:  eta: 0:02:42  iter: 59  total_loss: 1.682  loss_cls: 0.3272  loss_box_reg: 0.008804  loss_rpn_cls: 0.6854  loss_rpn_loc: 0.7622  time: 1.1451  data_time: 0.6216  lr: 2.972e-05  max_mem: 4859M\n",
      "[10/21 00:35:05 d2.utils.events]:  eta: 0:02:10  iter: 79  total_loss: 1.563  loss_cls: 0.3072  loss_box_reg: 0.02207  loss_rpn_cls: 0.6587  loss_rpn_loc: 0.5659  time: 1.2138  data_time: 0.7632  lr: 3.9711e-05  max_mem: 7466M\n",
      "[10/21 00:35:20 d2.utils.events]:  eta: 0:02:01  iter: 99  total_loss: 1.648  loss_cls: 0.2733  loss_box_reg: 0.0137  loss_rpn_cls: 0.6415  loss_rpn_loc: 0.6783  time: 1.1261  data_time: 0.5100  lr: 4.9701e-05  max_mem: 7466M\n",
      "[10/21 00:35:43 d2.utils.events]:  eta: 0:01:54  iter: 119  total_loss: 1.294  loss_cls: 0.1595  loss_box_reg: 0.01188  loss_rpn_cls: 0.5966  loss_rpn_loc: 0.4919  time: 1.1316  data_time: 0.8405  lr: 5.9691e-05  max_mem: 7466M\n",
      "[10/21 00:36:00 d2.utils.events]:  eta: 0:01:48  iter: 139  total_loss: 1.224  loss_cls: 0.1758  loss_box_reg: 0.04017  loss_rpn_cls: 0.5813  loss_rpn_loc: 0.3999  time: 1.0851  data_time: 0.4708  lr: 6.9681e-05  max_mem: 7466M\n",
      "[10/21 00:36:18 d2.utils.events]:  eta: 0:01:40  iter: 159  total_loss: 1.167  loss_cls: 0.1431  loss_box_reg: 0.02324  loss_rpn_cls: 0.5497  loss_rpn_loc: 0.4721  time: 1.0607  data_time: 0.6340  lr: 7.9671e-05  max_mem: 7466M\n",
      "[10/21 00:36:34 d2.utils.events]:  eta: 0:01:33  iter: 179  total_loss: 1.265  loss_cls: 0.221  loss_box_reg: 0.0892  loss_rpn_cls: 0.5323  loss_rpn_loc: 0.4105  time: 1.0359  data_time: 0.5716  lr: 8.966e-05  max_mem: 7466M\n",
      "[10/21 00:36:57 d2.utils.events]:  eta: 0:01:29  iter: 199  total_loss: 1.312  loss_cls: 0.2344  loss_box_reg: 0.103  loss_rpn_cls: 0.501  loss_rpn_loc: 0.4349  time: 1.0439  data_time: 0.8528  lr: 9.9651e-05  max_mem: 7466M\n",
      "[10/21 00:37:15 d2.utils.events]:  eta: 0:01:23  iter: 219  total_loss: 1.245  loss_cls: 0.2762  loss_box_reg: 0.1393  loss_rpn_cls: 0.4847  loss_rpn_loc: 0.3493  time: 1.0297  data_time: 0.5547  lr: 0.00010964  max_mem: 7466M\n",
      "[10/21 00:37:30 d2.utils.events]:  eta: 0:01:17  iter: 239  total_loss: 1.316  loss_cls: 0.2353  loss_box_reg: 0.1758  loss_rpn_cls: 0.453  loss_rpn_loc: 0.3071  time: 1.0070  data_time: 0.5006  lr: 0.00011963  max_mem: 7466M\n",
      "[10/21 00:37:48 d2.utils.events]:  eta: 0:01:11  iter: 259  total_loss: 1.278  loss_cls: 0.207  loss_box_reg: 0.175  loss_rpn_cls: 0.4027  loss_rpn_loc: 0.3226  time: 0.9979  data_time: 0.6137  lr: 0.00012962  max_mem: 7466M\n",
      "[10/21 00:38:07 d2.utils.events]:  eta: 0:01:05  iter: 279  total_loss: 1.391  loss_cls: 0.2943  loss_box_reg: 0.2077  loss_rpn_cls: 0.4262  loss_rpn_loc: 0.3861  time: 0.9954  data_time: 0.6938  lr: 0.00013961  max_mem: 7466M\n",
      "[10/21 00:38:23 d2.utils.events]:  eta: 0:01:00  iter: 299  total_loss: 1.354  loss_cls: 0.2958  loss_box_reg: 0.2909  loss_rpn_cls: 0.4026  loss_rpn_loc: 0.347  time: 0.9826  data_time: 0.4597  lr: 0.0001496  max_mem: 7466M\n",
      "[10/21 00:38:37 d2.utils.events]:  eta: 0:00:54  iter: 319  total_loss: 1.098  loss_cls: 0.2216  loss_box_reg: 0.2709  loss_rpn_cls: 0.3705  loss_rpn_loc: 0.2949  time: 0.9637  data_time: 0.4452  lr: 0.00015959  max_mem: 7466M\n",
      "[10/21 00:38:54 d2.utils.events]:  eta: 0:00:48  iter: 339  total_loss: 1.55  loss_cls: 0.2928  loss_box_reg: 0.3729  loss_rpn_cls: 0.3562  loss_rpn_loc: 0.345  time: 0.9567  data_time: 0.5707  lr: 0.00016958  max_mem: 7466M\n",
      "[10/21 00:39:12 d2.utils.events]:  eta: 0:00:42  iter: 359  total_loss: 1.081  loss_cls: 0.2374  loss_box_reg: 0.2364  loss_rpn_cls: 0.2952  loss_rpn_loc: 0.278  time: 0.9537  data_time: 0.6110  lr: 0.00017957  max_mem: 7466M\n",
      "[10/21 00:39:29 d2.utils.events]:  eta: 0:00:36  iter: 379  total_loss: 1.078  loss_cls: 0.2188  loss_box_reg: 0.2927  loss_rpn_cls: 0.286  loss_rpn_loc: 0.3244  time: 0.9495  data_time: 0.6182  lr: 0.00018956  max_mem: 7466M\n",
      "[10/21 00:39:52 d2.utils.events]:  eta: 0:00:30  iter: 399  total_loss: 1.162  loss_cls: 0.2269  loss_box_reg: 0.2687  loss_rpn_cls: 0.3451  loss_rpn_loc: 0.3155  time: 0.9595  data_time: 0.8741  lr: 0.00019955  max_mem: 7466M\n",
      "[10/21 00:40:04 d2.utils.events]:  eta: 0:00:24  iter: 419  total_loss: 1.048  loss_cls: 0.1797  loss_box_reg: 0.2142  loss_rpn_cls: 0.2784  loss_rpn_loc: 0.2865  time: 0.9411  data_time: 0.3476  lr: 0.00020954  max_mem: 7466M\n",
      "[10/21 00:40:22 d2.utils.events]:  eta: 0:00:18  iter: 439  total_loss: 1.114  loss_cls: 0.2294  loss_box_reg: 0.4086  loss_rpn_cls: 0.2324  loss_rpn_loc: 0.2981  time: 0.9409  data_time: 0.6598  lr: 0.00021953  max_mem: 7466M\n",
      "[10/21 00:40:41 d2.utils.events]:  eta: 0:00:12  iter: 459  total_loss: 1.174  loss_cls: 0.2443  loss_box_reg: 0.3811  loss_rpn_cls: 0.2632  loss_rpn_loc: 0.3216  time: 0.9414  data_time: 0.6534  lr: 0.00022952  max_mem: 7466M\n",
      "[10/21 00:41:03 d2.utils.events]:  eta: 0:00:06  iter: 479  total_loss: 1.194  loss_cls: 0.2327  loss_box_reg: 0.3626  loss_rpn_cls: 0.2639  loss_rpn_loc: 0.3682  time: 0.9480  data_time: 0.7703  lr: 0.00023951  max_mem: 7466M\n",
      "[10/21 00:41:27 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 1.024  loss_cls: 0.1553  loss_box_reg: 0.2476  loss_rpn_cls: 0.2643  loss_rpn_loc: 0.3235  time: 0.9558  data_time: 0.7870  lr: 0.0002495  max_mem: 7466M\n",
      "[10/21 00:41:27 d2.engine.hooks]: Overall training speed: 498 iterations in 0:07:55 (0.9558 s / it)\n",
      "[10/21 00:41:27 d2.engine.hooks]: Total training time: 0:07:57 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRVEiICco3SV"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VYCIXdMZvDYL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_hRCf86KGi5v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for i in range(3):\n",
    "    idx = random.randrange(0, len(test_set))\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=plane_metadata, scale=0.5, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D0wRdlcKo6BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 00:41:56 d2.evaluation.coco_evaluation]: Trying to convert 'validate' to COCO format ...\n",
      "WARNING [10/21 00:41:56 d2.data.datasets.coco]: Using previously cached COCO format annotations at './output/validate_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "[10/21 00:41:57 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 1596         |             |              |\n",
      "|    total    | 1596         |             |              |             |              |\n",
      "[10/21 00:41:57 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[10/21 00:41:57 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...\n",
      "[10/21 00:41:57 d2.data.common]: Serialized dataset takes 2.21 MiB\n",
      "[10/21 00:41:57 d2.evaluation.evaluator]: Start inference on 40 batches\n",
      "[10/21 00:42:01 d2.evaluation.evaluator]: Inference done 11/40. Dataloading: 0.0374 s/iter. Inference: 0.0528 s/iter. Eval: 0.0003 s/iter. Total: 0.0905 s/iter. ETA=0:00:02\n",
      "[10/21 00:42:06 d2.evaluation.evaluator]: Inference done 20/40. Dataloading: 0.3220 s/iter. Inference: 0.0637 s/iter. Eval: 0.0003 s/iter. Total: 0.3863 s/iter. ETA=0:00:07\n",
      "[10/21 00:42:13 d2.evaluation.evaluator]: Inference done 31/40. Dataloading: 0.4228 s/iter. Inference: 0.0746 s/iter. Eval: 0.0003 s/iter. Total: 0.4979 s/iter. ETA=0:00:04\n",
      "[10/21 00:42:15 d2.evaluation.evaluator]: Total inference time: 0:00:15.068664 (0.430533 s / iter per device, on 1 devices)\n",
      "[10/21 00:42:15 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:02 (0.069099 s / iter per device, on 1 devices)\n",
      "[10/21 00:42:15 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[10/21 00:42:15 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[10/21 00:42:15 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[10/21 00:42:15 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[10/21 00:42:15 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "[10/21 00:42:15 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[10/21 00:42:15 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.063\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.186\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.022\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.056\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.101\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.162\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333\n",
      "[10/21 00:42:15 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 6.335 | 18.588 | 1.813  | 7.157 | 8.989 | 2.207 |\n",
      "[10/21 00:42:15 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category    | AP   | category    | AP    | category    | AP   |\n",
      "|:------------|:-----|:------------|:------|:------------|:-----|\n",
      "| not plane 1 | nan  | not plane 2 | nan   | not plane 3 | nan  |\n",
      "| not plane 4 | nan  | plane       | 6.335 |             |      |\n",
      "OrderedDict([('bbox', {'AP': 6.335291924020003, 'AP50': 18.58805119040951, 'AP75': 1.8126174607639052, 'APs': 7.156708150211137, 'APm': 8.989014102281427, 'APl': 2.206690347238972, 'AP-not plane 1': nan, 'AP-not plane 2': nan, 'AP-not plane 3': nan, 'AP-not plane 4': nan, 'AP-plane': 6.335291924020003})])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"validate\", output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"validate\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwYvbwcjpKBk"
   },
   "source": [
    "### Improvements\n",
    "\n",
    "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdR6KbCZpOlk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Bring any changes and updates regarding the improvement in here\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_M4TooqSs2"
   },
   "source": [
    "## Part 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAEsMtIPLrO"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peQ95zLuIpkk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
    "# idx is the index of the target bbox in the data\n",
    "# high-resolution image could be passed or could be load from data['file_name']\n",
    "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask\n",
    "#     to convert the segmentation annotations to binary masks\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n",
    "def get_instance_sample(data, idx, img=None):\n",
    "\n",
    "  return obj_img, obj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxrc9X_pjzj-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# We have provided a template data loader for your segmentation training\n",
    "# You need to complete the __getitem__() function before running the code\n",
    "# You may also need to add data augmentation or normalization in here\n",
    "'''\n",
    "\n",
    "class PlaneDataset(Dataset):\n",
    "  def __init__(self, set_name, data_list):\n",
    "      self.transforms = transforms.Compose([\n",
    "          transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
    "      ])\n",
    "      self.set_name = set_name\n",
    "      self.data = data_list\n",
    "      self.instance_map = []\n",
    "      for i, d in enumerate(self.data):\n",
    "        for j in range(len(d['annotations'])):\n",
    "          self.instance_map.append([i,j])\n",
    "\n",
    "  '''\n",
    "  # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
    "  # make sure to use the correct length for the final training\n",
    "  '''\n",
    "  def __len__(self):\n",
    "      return len(self.instance_map)\n",
    "\n",
    "  def numpy_to_tensor(self, img, mask):\n",
    "    if self.transforms is not None:\n",
    "        img = self.transforms(img)\n",
    "    img = torch.tensor(img, dtype=torch.float)\n",
    "    mask = torch.tensor(mask, dtype=torch.float)\n",
    "    return img, mask\n",
    "\n",
    "  '''\n",
    "  # Complete this part by using get_instance_sample function\n",
    "  # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
    "  # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
    "  # TODO: 5 lines\n",
    "  '''\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "        idx = idx.tolist()\n",
    "    idx = self.instance_map[idx]\n",
    "    data = self.data[idx[0]]\n",
    "\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "def get_plane_dataset(set_name='train', batch_size=2):\n",
    "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
    "    dataset = PlaneDataset(set_name, my_data_list)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4,\n",
    "                                              pin_memory=True, shuffle=True)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eH3NKaQQfc"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeqR3s3dSBPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
    "'''\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation=True):\n",
    "        super(conv, self).__init__()\n",
    "        if(activation):\n",
    "          self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "             nn.BatchNorm2d(out_ch),\n",
    "             nn.ReLU(inplace=True)\n",
    "          )\n",
    "        else:\n",
    "          self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# downsampling module equal to a conv module followed by a max-pool layer\n",
    "'''\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            conv(in_ch, out_ch),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# upsampling module equal to a upsample function followed by a conv module\n",
    "'''\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.up(x)\n",
    "        y = self.conv(y)\n",
    "        return y\n",
    "\n",
    "'''\n",
    "# the main model which you need to complete by using above modules.\n",
    "# you can also modify the above modules in order to improve your results.\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        self.input_conv = conv(3, 4)\n",
    "        self.down = down(4, 8)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        self.up = up(8, 4)\n",
    "        self.output_conv = conv(4, 1, False) # ReLu activation is removed to keep the logits for the loss function\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "      y = self.input_conv(input)\n",
    "      y = self.down(y)\n",
    "      y = self.up(y)\n",
    "      output = self.output_conv(y)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj86vD9QT_Z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaZuO4SKSBuF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# The following is a basic training procedure to train the network\n",
    "# You need to update the code to get the best performance\n",
    "# TODO: approx ? lines\n",
    "'''\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = MyModel() # initialize the model\n",
    "model = model.cuda() # move the model to GPU\n",
    "loader, _ = get_plane_dataset('train', batch_size) # initialize data_loader\n",
    "crit = nn.BCEWithLogitsLoss() # Define the loss function\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
    "\n",
    "# start the training procedure\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0\n",
    "  for (img, mask) in tqdm(loader):\n",
    "    img = torch.tensor(img, device=torch.device('cuda'), requires_grad = True)\n",
    "    mask = torch.tensor(mask, device=torch.device('cuda'), requires_grad = True)\n",
    "    pred = model(img)\n",
    "    loss = crit(pred, mask)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.cpu().data\n",
    "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
    "  torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
    "\n",
    "'''\n",
    "# Saving the final model\n",
    "'''\n",
    "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyez1fyQYw7"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeViryUSCL2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Before starting the evaluation, you need to set the model mode to eval\n",
    "# You may load the trained model again, in case if you want to continue your code later\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "batch_size = 8\n",
    "model = MyModel().cuda()\n",
    "model.load_state_dict(torch.load('{}/output/final_segmentation_model.pth'.format(BASE_DIR)))\n",
    "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
    "loader, dataset = get_plane_dataset('train', batch_size)\n",
    "\n",
    "total_iou = 0\n",
    "for (img, mask) in tqdm(loader):\n",
    "  with torch.no_grad():\n",
    "    img = img.cuda()\n",
    "    mask = mask.cuda()\n",
    "    mask = torch.unsqueeze(mask,1)\n",
    "    pred = model(img)\n",
    "\n",
    "    '''\n",
    "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
    "    '''\n",
    "\n",
    "\n",
    "print(\"\\n #images: {}, Mean IoU: {}\".format(_, _))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbmTj9JICiKz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize 3 sample outputs\n",
    "# TODO: approx 5 lines\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navoiGdrqaZT"
   },
   "source": [
    "## Part 3: Instance Segmentation\n",
    "\n",
    "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwk33DGBowP"
   },
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrZ8TG-Ot2J"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define a new function to obtain the prediction mask by passing a sample data\n",
    "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
    "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
    "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "\n",
    "def get_prediction_mask(data):\n",
    "\n",
    "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc7TSK6EBi9u"
   },
   "source": [
    "### Visualization and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T2YX8MBiGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPo_03up-g_f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
    "'''\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
    "    if(len(dots)==0):\n",
    "      return []\n",
    "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
    "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
    "    tmpdots = dots[inds]\n",
    "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
    "    inds = inds[1:] - inds[:-1]\n",
    "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
    "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
    "    return ' '.join([str(i) for i in runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv0rab2LJev-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# You need to upload the csv file on kaggle\n",
    "# The speed of your code in the previous parts highly affects the running time of this part\n",
    "'''\n",
    "\n",
    "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the training set\n",
    "'''\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for index in inds:\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index)\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the test set\n",
    "'''\n",
    "\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for j, index in enumerate(inds):\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index).double()\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
    "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nN4SduqrpI"
   },
   "source": [
    "## Part 4: Mask R-CNN\n",
    "\n",
    "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWf7drKNXYd"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1FDCcQN2LH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG5slAhQNjE7"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1pnKLMOcjS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ifeV1sNvtt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4K0Nz5OeKk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298QruFnNxyn"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcGwV5-9Oetp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_3wS2BFLLGp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
