{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "This is a template notebook for Assignment 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## Install dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-i4hmGYk1dL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some common libraries\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import some common pytorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tUA_j6AF1L5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that GPU is available for your notebook.\n",
    "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A_Di_fgL4HSv"
   },
   "outputs": [],
   "source": [
    "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
    "# TODO: approx 1 line\n",
    "BASE_DIR = '.'\n",
    "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## Part 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRV-KFJzlur4"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(os.path.join(data_dirs, \"train.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"validate\" else train_annotations\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCH-2mWxDVVu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"validate\", \"test\"]:\n",
    "    DatasetCatalog.register(i, lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(i).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qNSdXCL_DVAz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "plane_metadata = MetadataCatalog.get(\"train\")\n",
    "train_set = get_detection_data(\"train\")\n",
    "data = train_set[random.randrange(0, len(train_set))]\n",
    "img = cv2.imread(data[\"file_name\"])\n",
    "visualizer = Visualizer(img[:, :, ::-1], metadata=plane_metadata, scale=0.5)\n",
    "out = visualizer.draw_dataset_dict(data)\n",
    "save_path = os.path.join(BASE_DIR, \"output\", \"train_set.jpg\")\n",
    "cv2.imwrite(save_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4rql8pNokE4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7d3KxiHO_0gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 01:59:23 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 01:59:24 d2.data.build]: Removed 0 images with no usable annotations. 159 images left.\n",
      "[10/21 01:59:24 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[10/21 01:59:24 d2.data.build]: Using training sampler TrainingSampler\n",
      "[10/21 01:59:24 d2.data.common]: Serializing 159 elements to byte tensors and concatenating them all ...\n",
      "[10/21 01:59:24 d2.data.common]: Serialized dataset takes 13.26 MiB\n",
      "WARNING [10/21 01:59:24 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "[10/21 01:59:24 d2.engine.train_loop]: Starting training from iteration 0\n",
      "[10/21 01:59:47 d2.utils.events]:  eta: 0:02:12  iter: 19  total_loss: 3.052  loss_cls: 1.03  loss_box_reg: 0.3042  loss_rpn_cls: 1.514  loss_rpn_loc: 0.331  time: 1.0453  data_time: 0.8469  lr: 9.7405e-06  max_mem: 7466M\n",
      "[10/21 02:00:06 d2.utils.events]:  eta: 0:02:10  iter: 39  total_loss: 2.132  loss_cls: 0.5942  loss_box_reg: 0.2193  loss_rpn_cls: 0.5715  loss_rpn_loc: 0.4401  time: 0.9967  data_time: 0.5851  lr: 1.9731e-05  max_mem: 7466M\n",
      "[10/21 02:00:24 d2.utils.events]:  eta: 0:02:12  iter: 59  total_loss: 1.258  loss_cls: 0.3227  loss_box_reg: 0.2302  loss_rpn_cls: 0.2844  loss_rpn_loc: 0.3484  time: 0.9558  data_time: 0.6325  lr: 2.972e-05  max_mem: 7466M\n",
      "[10/21 02:00:39 d2.utils.events]:  eta: 0:02:08  iter: 79  total_loss: 1.045  loss_cls: 0.2907  loss_box_reg: 0.1973  loss_rpn_cls: 0.2106  loss_rpn_loc: 0.2506  time: 0.9027  data_time: 0.4085  lr: 3.9711e-05  max_mem: 7466M\n",
      "[10/21 02:00:59 d2.utils.events]:  eta: 0:01:57  iter: 99  total_loss: 1.062  loss_cls: 0.2991  loss_box_reg: 0.2467  loss_rpn_cls: 0.2577  loss_rpn_loc: 0.3122  time: 0.9199  data_time: 0.6835  lr: 4.9701e-05  max_mem: 7466M\n",
      "[10/21 02:01:15 d2.utils.events]:  eta: 0:01:45  iter: 119  total_loss: 0.8807  loss_cls: 0.1647  loss_box_reg: 0.1677  loss_rpn_cls: 0.2277  loss_rpn_loc: 0.3278  time: 0.9032  data_time: 0.5600  lr: 5.9691e-05  max_mem: 7466M\n",
      "[10/21 02:01:33 d2.utils.events]:  eta: 0:01:39  iter: 139  total_loss: 0.9945  loss_cls: 0.1853  loss_box_reg: 0.2078  loss_rpn_cls: 0.1791  loss_rpn_loc: 0.2666  time: 0.8996  data_time: 0.6142  lr: 6.9681e-05  max_mem: 7466M\n",
      "[10/21 02:01:50 d2.utils.events]:  eta: 0:01:33  iter: 159  total_loss: 0.9377  loss_cls: 0.1877  loss_box_reg: 0.2303  loss_rpn_cls: 0.1903  loss_rpn_loc: 0.2842  time: 0.8943  data_time: 0.5969  lr: 7.9671e-05  max_mem: 7466M\n",
      "[10/21 02:02:07 d2.utils.events]:  eta: 0:01:28  iter: 179  total_loss: 0.9113  loss_cls: 0.1725  loss_box_reg: 0.2532  loss_rpn_cls: 0.197  loss_rpn_loc: 0.2462  time: 0.8885  data_time: 0.5336  lr: 8.966e-05  max_mem: 7466M\n",
      "[10/21 02:02:22 d2.utils.events]:  eta: 0:01:23  iter: 199  total_loss: 0.8616  loss_cls: 0.1721  loss_box_reg: 0.23  loss_rpn_cls: 0.1778  loss_rpn_loc: 0.2597  time: 0.8749  data_time: 0.5120  lr: 9.9651e-05  max_mem: 7466M\n",
      "[10/21 02:02:38 d2.utils.events]:  eta: 0:01:17  iter: 219  total_loss: 0.9495  loss_cls: 0.1707  loss_box_reg: 0.238  loss_rpn_cls: 0.191  loss_rpn_loc: 0.271  time: 0.8704  data_time: 0.5453  lr: 0.00010964  max_mem: 7466M\n",
      "[10/21 02:02:59 d2.utils.events]:  eta: 0:01:12  iter: 239  total_loss: 0.7495  loss_cls: 0.1439  loss_box_reg: 0.2714  loss_rpn_cls: 0.1438  loss_rpn_loc: 0.1944  time: 0.8852  data_time: 0.7807  lr: 0.00011963  max_mem: 7466M\n",
      "[10/21 02:03:14 d2.utils.events]:  eta: 0:01:06  iter: 259  total_loss: 0.9639  loss_cls: 0.2116  loss_box_reg: 0.3275  loss_rpn_cls: 0.1832  loss_rpn_loc: 0.2002  time: 0.8738  data_time: 0.4740  lr: 0.00012962  max_mem: 7466M\n",
      "[10/21 02:03:31 d2.utils.events]:  eta: 0:01:00  iter: 279  total_loss: 0.9046  loss_cls: 0.188  loss_box_reg: 0.3  loss_rpn_cls: 0.1643  loss_rpn_loc: 0.2361  time: 0.8722  data_time: 0.5995  lr: 0.00013961  max_mem: 7466M\n",
      "[10/21 02:03:52 d2.utils.events]:  eta: 0:00:55  iter: 299  total_loss: 0.7244  loss_cls: 0.1576  loss_box_reg: 0.245  loss_rpn_cls: 0.1325  loss_rpn_loc: 0.1895  time: 0.8840  data_time: 0.7047  lr: 0.0001496  max_mem: 7466M\n",
      "[10/21 02:04:07 d2.utils.events]:  eta: 0:00:49  iter: 319  total_loss: 0.8248  loss_cls: 0.1635  loss_box_reg: 0.2394  loss_rpn_cls: 0.17  loss_rpn_loc: 0.2638  time: 0.8742  data_time: 0.4863  lr: 0.00015959  max_mem: 7466M\n",
      "[10/21 02:04:20 d2.utils.events]:  eta: 0:00:44  iter: 339  total_loss: 0.754  loss_cls: 0.1346  loss_box_reg: 0.1913  loss_rpn_cls: 0.1598  loss_rpn_loc: 0.2084  time: 0.8625  data_time: 0.3780  lr: 0.00016958  max_mem: 7466M\n",
      "[10/21 02:04:38 d2.utils.events]:  eta: 0:00:39  iter: 359  total_loss: 0.7758  loss_cls: 0.157  loss_box_reg: 0.271  loss_rpn_cls: 0.1106  loss_rpn_loc: 0.2368  time: 0.8650  data_time: 0.6375  lr: 0.00017957  max_mem: 7466M\n",
      "[10/21 02:04:57 d2.utils.events]:  eta: 0:00:33  iter: 379  total_loss: 0.6968  loss_cls: 0.1544  loss_box_reg: 0.2791  loss_rpn_cls: 0.108  loss_rpn_loc: 0.1725  time: 0.8673  data_time: 0.6352  lr: 0.00018956  max_mem: 7466M\n",
      "[10/21 02:05:16 d2.utils.events]:  eta: 0:00:28  iter: 399  total_loss: 0.8211  loss_cls: 0.1882  loss_box_reg: 0.2261  loss_rpn_cls: 0.1271  loss_rpn_loc: 0.2636  time: 0.8719  data_time: 0.6675  lr: 0.00019955  max_mem: 7466M\n",
      "[10/21 02:05:30 d2.utils.events]:  eta: 0:00:22  iter: 419  total_loss: 0.7344  loss_cls: 0.1417  loss_box_reg: 0.2495  loss_rpn_cls: 0.1055  loss_rpn_loc: 0.1529  time: 0.8640  data_time: 0.4649  lr: 0.00020954  max_mem: 7466M\n",
      "[10/21 02:05:49 d2.utils.events]:  eta: 0:00:17  iter: 439  total_loss: 0.8885  loss_cls: 0.2045  loss_box_reg: 0.2416  loss_rpn_cls: 0.124  loss_rpn_loc: 0.2496  time: 0.8676  data_time: 0.5825  lr: 0.00021953  max_mem: 7466M\n",
      "[10/21 02:06:09 d2.utils.events]:  eta: 0:00:11  iter: 459  total_loss: 0.7561  loss_cls: 0.1497  loss_box_reg: 0.26  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.1652  time: 0.8740  data_time: 0.7630  lr: 0.00022952  max_mem: 7466M\n",
      "[10/21 02:06:21 d2.utils.events]:  eta: 0:00:05  iter: 479  total_loss: 0.7616  loss_cls: 0.1445  loss_box_reg: 0.2333  loss_rpn_cls: 0.1056  loss_rpn_loc: 0.2449  time: 0.8621  data_time: 0.3186  lr: 0.00023951  max_mem: 7466M\n",
      "[10/21 02:06:42 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.7736  loss_cls: 0.1546  loss_box_reg: 0.2601  loss_rpn_cls: 0.08178  loss_rpn_loc: 0.2061  time: 0.8686  data_time: 0.7137  lr: 0.0002495  max_mem: 7466M\n",
      "[10/21 02:06:42 d2.engine.hooks]: Overall training speed: 498 iterations in 0:07:12 (0.8686 s / it)\n",
      "[10/21 02:06:42 d2.engine.hooks]: Total training time: 0:07:13 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRVEiICco3SV"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VYCIXdMZvDYL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_hRCf86KGi5v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for i in range(3):\n",
    "    idx = random.randrange(0, len(test_set))\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=plane_metadata, scale=0.5, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "D0wRdlcKo6BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 01:56:50 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[10/21 01:56:50 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...\n",
      "[10/21 01:56:50 d2.data.common]: Serialized dataset takes 2.21 MiB\n",
      "[10/21 01:56:50 d2.evaluation.evaluator]: Start inference on 40 batches\n",
      "[10/21 01:56:54 d2.evaluation.evaluator]: Inference done 11/40. Dataloading: 0.0317 s/iter. Inference: 0.0439 s/iter. Eval: 0.0002 s/iter. Total: 0.0758 s/iter. ETA=0:00:02\n",
      "[10/21 01:56:59 d2.evaluation.evaluator]: Inference done 20/40. Dataloading: 0.3188 s/iter. Inference: 0.0674 s/iter. Eval: 0.0002 s/iter. Total: 0.3867 s/iter. ETA=0:00:07\n",
      "[10/21 01:57:06 d2.evaluation.evaluator]: Inference done 31/40. Dataloading: 0.3967 s/iter. Inference: 0.0721 s/iter. Eval: 0.0002 s/iter. Total: 0.4692 s/iter. ETA=0:00:04\n",
      "[10/21 01:57:08 d2.evaluation.evaluator]: Total inference time: 0:00:14.399571 (0.411416 s / iter per device, on 1 devices)\n",
      "[10/21 01:57:08 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:02 (0.068615 s / iter per device, on 1 devices)\n",
      "[10/21 01:57:08 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[10/21 01:57:08 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[10/21 01:57:08 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[10/21 01:57:08 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[10/21 01:57:08 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[10/21 01:57:08 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[10/21 01:57:08 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "[10/21 01:57:08 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.053 | 1.980  | 0.517  | 0.693 | 3.012 | 0.000 |\n",
      "[10/21 01:57:08 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category    | AP   | category    | AP    | category    | AP   |\n",
      "|:------------|:-----|:------------|:------|:------------|:-----|\n",
      "| not plane 1 | nan  | not plane 2 | nan   | not plane 3 | nan  |\n",
      "| not plane 4 | nan  | plane       | 1.053 |             |      |\n",
      "OrderedDict([('bbox', {'AP': 1.0533727285772057, 'AP50': 1.9801980198019802, 'AP75': 0.5165733964700818, 'APs': 0.6930693069306931, 'APm': 3.0121147742304593, 'APl': 0.0, 'AP-not plane 1': nan, 'AP-not plane 2': nan, 'AP-not plane 3': nan, 'AP-not plane 4': nan, 'AP-plane': 1.0533727285772057})])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"validate\", output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"validate\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwYvbwcjpKBk"
   },
   "source": [
    "### Improvements\n",
    "\n",
    "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdR6KbCZpOlk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Bring any changes and updates regarding the improvement in here\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_M4TooqSs2"
   },
   "source": [
    "## Part 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAEsMtIPLrO"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peQ95zLuIpkk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
    "# idx is the index of the target bbox in the data\n",
    "# high-resolution image could be passed or could be load from data['file_name']\n",
    "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask\n",
    "#     to convert the segmentation annotations to binary masks\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n",
    "def get_instance_sample(data, idx, img=None):\n",
    "\n",
    "  return obj_img, obj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxrc9X_pjzj-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# We have provided a template data loader for your segmentation training\n",
    "# You need to complete the __getitem__() function before running the code\n",
    "# You may also need to add data augmentation or normalization in here\n",
    "'''\n",
    "\n",
    "class PlaneDataset(Dataset):\n",
    "  def __init__(self, set_name, data_list):\n",
    "      self.transforms = transforms.Compose([\n",
    "          transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
    "      ])\n",
    "      self.set_name = set_name\n",
    "      self.data = data_list\n",
    "      self.instance_map = []\n",
    "      for i, d in enumerate(self.data):\n",
    "        for j in range(len(d['annotations'])):\n",
    "          self.instance_map.append([i,j])\n",
    "\n",
    "  '''\n",
    "  # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
    "  # make sure to use the correct length for the final training\n",
    "  '''\n",
    "  def __len__(self):\n",
    "      return len(self.instance_map)\n",
    "\n",
    "  def numpy_to_tensor(self, img, mask):\n",
    "    if self.transforms is not None:\n",
    "        img = self.transforms(img)\n",
    "    img = torch.tensor(img, dtype=torch.float)\n",
    "    mask = torch.tensor(mask, dtype=torch.float)\n",
    "    return img, mask\n",
    "\n",
    "  '''\n",
    "  # Complete this part by using get_instance_sample function\n",
    "  # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
    "  # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
    "  # TODO: 5 lines\n",
    "  '''\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "        idx = idx.tolist()\n",
    "    idx = self.instance_map[idx]\n",
    "    data = self.data[idx[0]]\n",
    "\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "def get_plane_dataset(set_name='train', batch_size=2):\n",
    "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
    "    dataset = PlaneDataset(set_name, my_data_list)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4,\n",
    "                                              pin_memory=True, shuffle=True)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eH3NKaQQfc"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeqR3s3dSBPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
    "'''\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation=True):\n",
    "        super(conv, self).__init__()\n",
    "        if(activation):\n",
    "          self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "             nn.BatchNorm2d(out_ch),\n",
    "             nn.ReLU(inplace=True)\n",
    "          )\n",
    "        else:\n",
    "          self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# downsampling module equal to a conv module followed by a max-pool layer\n",
    "'''\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            conv(in_ch, out_ch),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# upsampling module equal to a upsample function followed by a conv module\n",
    "'''\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.up(x)\n",
    "        y = self.conv(y)\n",
    "        return y\n",
    "\n",
    "'''\n",
    "# the main model which you need to complete by using above modules.\n",
    "# you can also modify the above modules in order to improve your results.\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        self.input_conv = conv(3, 4)\n",
    "        self.down = down(4, 8)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        self.up = up(8, 4)\n",
    "        self.output_conv = conv(4, 1, False) # ReLu activation is removed to keep the logits for the loss function\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "      y = self.input_conv(input)\n",
    "      y = self.down(y)\n",
    "      y = self.up(y)\n",
    "      output = self.output_conv(y)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj86vD9QT_Z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaZuO4SKSBuF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# The following is a basic training procedure to train the network\n",
    "# You need to update the code to get the best performance\n",
    "# TODO: approx ? lines\n",
    "'''\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = MyModel() # initialize the model\n",
    "model = model.cuda() # move the model to GPU\n",
    "loader, _ = get_plane_dataset('train', batch_size) # initialize data_loader\n",
    "crit = nn.BCEWithLogitsLoss() # Define the loss function\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
    "\n",
    "# start the training procedure\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0\n",
    "  for (img, mask) in tqdm(loader):\n",
    "    img = torch.tensor(img, device=torch.device('cuda'), requires_grad = True)\n",
    "    mask = torch.tensor(mask, device=torch.device('cuda'), requires_grad = True)\n",
    "    pred = model(img)\n",
    "    loss = crit(pred, mask)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.cpu().data\n",
    "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
    "  torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
    "\n",
    "'''\n",
    "# Saving the final model\n",
    "'''\n",
    "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyez1fyQYw7"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeViryUSCL2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Before starting the evaluation, you need to set the model mode to eval\n",
    "# You may load the trained model again, in case if you want to continue your code later\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "batch_size = 8\n",
    "model = MyModel().cuda()\n",
    "model.load_state_dict(torch.load('{}/output/final_segmentation_model.pth'.format(BASE_DIR)))\n",
    "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
    "loader, dataset = get_plane_dataset('train', batch_size)\n",
    "\n",
    "total_iou = 0\n",
    "for (img, mask) in tqdm(loader):\n",
    "  with torch.no_grad():\n",
    "    img = img.cuda()\n",
    "    mask = mask.cuda()\n",
    "    mask = torch.unsqueeze(mask,1)\n",
    "    pred = model(img)\n",
    "\n",
    "    '''\n",
    "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
    "    '''\n",
    "\n",
    "\n",
    "print(\"\\n #images: {}, Mean IoU: {}\".format(_, _))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbmTj9JICiKz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize 3 sample outputs\n",
    "# TODO: approx 5 lines\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navoiGdrqaZT"
   },
   "source": [
    "## Part 3: Instance Segmentation\n",
    "\n",
    "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwk33DGBowP"
   },
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrZ8TG-Ot2J"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define a new function to obtain the prediction mask by passing a sample data\n",
    "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
    "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
    "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "\n",
    "def get_prediction_mask(data):\n",
    "\n",
    "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc7TSK6EBi9u"
   },
   "source": [
    "### Visualization and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T2YX8MBiGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPo_03up-g_f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
    "'''\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
    "    if(len(dots)==0):\n",
    "      return []\n",
    "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
    "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
    "    tmpdots = dots[inds]\n",
    "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
    "    inds = inds[1:] - inds[:-1]\n",
    "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
    "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
    "    return ' '.join([str(i) for i in runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv0rab2LJev-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# You need to upload the csv file on kaggle\n",
    "# The speed of your code in the previous parts highly affects the running time of this part\n",
    "'''\n",
    "\n",
    "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the training set\n",
    "'''\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for index in inds:\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index)\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the test set\n",
    "'''\n",
    "\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for j, index in enumerate(inds):\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index).double()\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
    "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nN4SduqrpI"
   },
   "source": [
    "## Part 4: Mask R-CNN\n",
    "\n",
    "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWf7drKNXYd"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1FDCcQN2LH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG5slAhQNjE7"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1pnKLMOcjS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ifeV1sNvtt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4K0Nz5OeKk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298QruFnNxyn"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcGwV5-9Oetp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_3wS2BFLLGp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
