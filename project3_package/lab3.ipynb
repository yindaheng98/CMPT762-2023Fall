{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "This is a template notebook for Assignment 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## Install dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-i4hmGYk1dL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some common libraries\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import some common pytorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tUA_j6AF1L5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that GPU is available for your notebook.\n",
    "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A_Di_fgL4HSv"
   },
   "outputs": [],
   "source": [
    "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
    "# TODO: approx 1 line\n",
    "BASE_DIR = '.'\n",
    "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## Part 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRV-KFJzlur4"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(os.path.join(data_dirs, \"train.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"val\" else train_annotations\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCH-2mWxDVVu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(\"data_detection_{}\".format(i), lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(\"data_detection_{}\".format(i)).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qNSdXCL_DVAz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "train_set = get_detection_data(\"train\")\n",
    "data = train_set[random.randrange(0, len(train_set))]\n",
    "img = cv2.imread(data[\"file_name\"])\n",
    "visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5)\n",
    "out = visualizer.draw_dataset_dict(data)\n",
    "save_path = os.path.join(BASE_DIR, \"output\", \"train_set.jpg\")\n",
    "cv2.imwrite(save_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"data_detection_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4rql8pNokE4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7d3KxiHO_0gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 02:54:22 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 02:54:23 d2.data.build]: Removed 0 images with no usable annotations. 159 images left.\n",
      "[10/21 02:54:23 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 6384         |             |              |\n",
      "|    total    | 6384         |             |              |             |              |\n",
      "[10/21 02:54:23 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[10/21 02:54:23 d2.data.build]: Using training sampler TrainingSampler\n",
      "[10/21 02:54:23 d2.data.common]: Serializing 159 elements to byte tensors and concatenating them all ...\n",
      "[10/21 02:54:23 d2.data.common]: Serialized dataset takes 13.26 MiB\n",
      "WARNING [10/21 02:54:23 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "[10/21 02:54:24 d2.engine.train_loop]: Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 02:54:57 d2.utils.events]:  eta: 0:04:02  iter: 19  total_loss: 4.153  loss_cls: 1.161  loss_box_reg: 0.275  loss_rpn_cls: 1.762  loss_rpn_loc: 0.3136  time: 1.1945  data_time: 1.2246  lr: 9.7405e-06  max_mem: 5370M\n",
      "[10/21 02:55:10 d2.utils.events]:  eta: 0:02:55  iter: 39  total_loss: 2.106  loss_cls: 0.7849  loss_box_reg: 0.2162  loss_rpn_cls: 0.5781  loss_rpn_loc: 0.457  time: 0.8972  data_time: 0.4005  lr: 1.9731e-05  max_mem: 5370M\n",
      "[10/21 02:55:23 d2.utils.events]:  eta: 0:02:20  iter: 59  total_loss: 1.245  loss_cls: 0.3449  loss_box_reg: 0.2225  loss_rpn_cls: 0.2593  loss_rpn_loc: 0.3557  time: 0.8254  data_time: 0.4332  lr: 2.972e-05  max_mem: 5370M\n",
      "[10/21 02:55:44 d2.utils.events]:  eta: 0:02:09  iter: 79  total_loss: 0.8636  loss_cls: 0.2197  loss_box_reg: 0.2175  loss_rpn_cls: 0.2964  loss_rpn_loc: 0.2688  time: 0.8716  data_time: 0.7335  lr: 3.9711e-05  max_mem: 5370M\n",
      "[10/21 02:56:00 d2.utils.events]:  eta: 0:02:05  iter: 99  total_loss: 1.086  loss_cls: 0.2  loss_box_reg: 0.2172  loss_rpn_cls: 0.2313  loss_rpn_loc: 0.2886  time: 0.8623  data_time: 0.5861  lr: 4.9701e-05  max_mem: 5370M\n",
      "[10/21 02:56:23 d2.utils.events]:  eta: 0:01:58  iter: 119  total_loss: 0.9983  loss_cls: 0.202  loss_box_reg: 0.2654  loss_rpn_cls: 0.1732  loss_rpn_loc: 0.2887  time: 0.9142  data_time: 0.8934  lr: 5.9691e-05  max_mem: 5370M\n",
      "[10/21 02:56:43 d2.utils.events]:  eta: 0:01:50  iter: 139  total_loss: 0.9177  loss_cls: 0.1743  loss_box_reg: 0.2072  loss_rpn_cls: 0.1846  loss_rpn_loc: 0.293  time: 0.9223  data_time: 0.6951  lr: 6.9681e-05  max_mem: 5370M\n",
      "[10/21 02:57:01 d2.utils.events]:  eta: 0:01:43  iter: 159  total_loss: 0.932  loss_cls: 0.1759  loss_box_reg: 0.2756  loss_rpn_cls: 0.1905  loss_rpn_loc: 0.281  time: 0.9225  data_time: 0.4373  lr: 7.9671e-05  max_mem: 6923M\n",
      "[10/21 02:57:22 d2.utils.events]:  eta: 0:01:38  iter: 179  total_loss: 0.8694  loss_cls: 0.1936  loss_box_reg: 0.2785  loss_rpn_cls: 0.1654  loss_rpn_loc: 0.2539  time: 0.9339  data_time: 0.7613  lr: 8.966e-05  max_mem: 6923M\n",
      "[10/21 02:57:38 d2.utils.events]:  eta: 0:01:31  iter: 199  total_loss: 1.086  loss_cls: 0.2272  loss_box_reg: 0.3355  loss_rpn_cls: 0.209  loss_rpn_loc: 0.228  time: 0.9199  data_time: 0.5515  lr: 9.9651e-05  max_mem: 6923M\n",
      "[10/21 02:57:55 d2.utils.events]:  eta: 0:01:23  iter: 219  total_loss: 0.7942  loss_cls: 0.1553  loss_box_reg: 0.1878  loss_rpn_cls: 0.1403  loss_rpn_loc: 0.2562  time: 0.9163  data_time: 0.5933  lr: 0.00010964  max_mem: 6923M\n",
      "[10/21 02:58:13 d2.utils.events]:  eta: 0:01:19  iter: 239  total_loss: 0.9169  loss_cls: 0.2142  loss_box_reg: 0.2556  loss_rpn_cls: 0.1727  loss_rpn_loc: 0.2127  time: 0.9111  data_time: 0.6195  lr: 0.00011963  max_mem: 6923M\n",
      "[10/21 02:58:29 d2.utils.events]:  eta: 0:01:09  iter: 259  total_loss: 0.9605  loss_cls: 0.2084  loss_box_reg: 0.3159  loss_rpn_cls: 0.1877  loss_rpn_loc: 0.2707  time: 0.9026  data_time: 0.5450  lr: 0.00012962  max_mem: 6923M\n",
      "[10/21 02:58:44 d2.utils.events]:  eta: 0:01:03  iter: 279  total_loss: 0.8299  loss_cls: 0.1621  loss_box_reg: 0.2692  loss_rpn_cls: 0.134  loss_rpn_loc: 0.2352  time: 0.8919  data_time: 0.4981  lr: 0.00013961  max_mem: 6923M\n",
      "[10/21 02:59:00 d2.utils.events]:  eta: 0:00:57  iter: 299  total_loss: 0.9078  loss_cls: 0.1654  loss_box_reg: 0.2818  loss_rpn_cls: 0.1805  loss_rpn_loc: 0.2613  time: 0.8862  data_time: 0.5035  lr: 0.0001496  max_mem: 6923M\n",
      "[10/21 02:59:16 d2.utils.events]:  eta: 0:00:52  iter: 319  total_loss: 0.7385  loss_cls: 0.1611  loss_box_reg: 0.2373  loss_rpn_cls: 0.1297  loss_rpn_loc: 0.2006  time: 0.8804  data_time: 0.5423  lr: 0.00015959  max_mem: 6923M\n",
      "[10/21 02:59:32 d2.utils.events]:  eta: 0:00:47  iter: 339  total_loss: 0.8315  loss_cls: 0.1849  loss_box_reg: 0.2839  loss_rpn_cls: 0.1352  loss_rpn_loc: 0.1997  time: 0.8753  data_time: 0.5518  lr: 0.00016958  max_mem: 6923M\n",
      "[10/21 02:59:58 d2.utils.events]:  eta: 0:00:40  iter: 359  total_loss: 0.7106  loss_cls: 0.1501  loss_box_reg: 0.2738  loss_rpn_cls: 0.1195  loss_rpn_loc: 0.2578  time: 0.8990  data_time: 0.6949  lr: 0.00017957  max_mem: 7185M\n",
      "[10/21 03:00:17 d2.utils.events]:  eta: 0:00:35  iter: 379  total_loss: 0.7725  loss_cls: 0.1609  loss_box_reg: 0.3275  loss_rpn_cls: 0.1347  loss_rpn_loc: 0.1884  time: 0.9036  data_time: 0.7204  lr: 0.00018956  max_mem: 7185M\n",
      "[10/21 03:00:37 d2.utils.events]:  eta: 0:00:29  iter: 399  total_loss: 0.9172  loss_cls: 0.1815  loss_box_reg: 0.2682  loss_rpn_cls: 0.1482  loss_rpn_loc: 0.2677  time: 0.9077  data_time: 0.7052  lr: 0.00019955  max_mem: 7185M\n",
      "[10/21 03:01:01 d2.utils.events]:  eta: 0:00:24  iter: 419  total_loss: 0.7976  loss_cls: 0.16  loss_box_reg: 0.2852  loss_rpn_cls: 0.1262  loss_rpn_loc: 0.2635  time: 0.9202  data_time: 0.8878  lr: 0.00020954  max_mem: 7185M\n",
      "[10/21 03:01:30 d2.utils.events]:  eta: 0:00:18  iter: 439  total_loss: 0.88  loss_cls: 0.1923  loss_box_reg: 0.2896  loss_rpn_cls: 0.1311  loss_rpn_loc: 0.2505  time: 0.9456  data_time: 1.1752  lr: 0.00021953  max_mem: 7185M\n",
      "[10/21 03:01:55 d2.utils.events]:  eta: 0:00:12  iter: 459  total_loss: 0.8147  loss_cls: 0.1716  loss_box_reg: 0.2519  loss_rpn_cls: 0.119  loss_rpn_loc: 0.19  time: 0.9588  data_time: 0.9692  lr: 0.00022952  max_mem: 7185M\n",
      "[10/21 03:02:09 d2.utils.events]:  eta: 0:00:06  iter: 479  total_loss: 0.5876  loss_cls: 0.121  loss_box_reg: 0.2061  loss_rpn_cls: 0.09525  loss_rpn_loc: 0.1191  time: 0.9479  data_time: 0.3848  lr: 0.00023951  max_mem: 7185M\n",
      "[10/21 03:02:30 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.8006  loss_cls: 0.1773  loss_box_reg: 0.298  loss_rpn_cls: 0.08476  loss_rpn_loc: 0.1659  time: 0.9506  data_time: 0.6721  lr: 0.0002495  max_mem: 7185M\n",
      "[10/21 03:02:30 d2.engine.hooks]: Overall training speed: 498 iterations in 0:07:53 (0.9506 s / it)\n",
      "[10/21 03:02:30 d2.engine.hooks]: Total training time: 0:07:54 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRVEiICco3SV"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VYCIXdMZvDYL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_hRCf86KGi5v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for i in range(3):\n",
    "    idx = random.randrange(0, len(test_set))\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "D0wRdlcKo6BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:04:41 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[10/21 03:04:41 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...\n",
      "[10/21 03:04:41 d2.data.common]: Serialized dataset takes 2.21 MiB\n",
      "[10/21 03:04:41 d2.evaluation.evaluator]: Start inference on 40 batches\n",
      "[10/21 03:04:45 d2.evaluation.evaluator]: Inference done 11/40. Dataloading: 0.0295 s/iter. Inference: 0.0480 s/iter. Eval: 0.0003 s/iter. Total: 0.0779 s/iter. ETA=0:00:02\n",
      "[10/21 03:04:50 d2.evaluation.evaluator]: Inference done 29/40. Dataloading: 0.1730 s/iter. Inference: 0.0545 s/iter. Eval: 0.0003 s/iter. Total: 0.2280 s/iter. ETA=0:00:02\n",
      "[10/21 03:04:54 d2.evaluation.evaluator]: Total inference time: 0:00:09.908469 (0.283099 s / iter per device, on 1 devices)\n",
      "[10/21 03:04:54 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:02 (0.058655 s / iter per device, on 1 devices)\n",
      "[10/21 03:04:54 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[10/21 03:04:54 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[10/21 03:04:54 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[10/21 03:04:54 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[10/21 03:04:54 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "[10/21 03:04:54 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[10/21 03:04:54 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.450\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.264\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.198\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.434\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.571\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.196\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.725\n",
      "[10/21 03:04:54 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 25.300 | 44.968 | 26.418 | 19.795 | 43.355 | 57.055 |\n",
      "[10/21 03:04:54 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category    | AP   | category    | AP     | category    | AP   |\n",
      "|:------------|:-----|:------------|:-------|:------------|:-----|\n",
      "| not plane 1 | nan  | not plane 2 | nan    | not plane 3 | nan  |\n",
      "| not plane 4 | nan  | plane       | 25.300 |             |      |\n",
      "OrderedDict([('bbox', {'AP': 25.30024834741496, 'AP50': 44.968221051209454, 'AP75': 26.418336519421086, 'APs': 19.79491324235279, 'APm': 43.35535731399964, 'APl': 57.05546468518645, 'AP-not plane 1': nan, 'AP-not plane 2': nan, 'AP-not plane 3': nan, 'AP-not plane 4': nan, 'AP-plane': 25.30024834741496})])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"data_detection_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwYvbwcjpKBk"
   },
   "source": [
    "### Improvements\n",
    "\n",
    "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdR6KbCZpOlk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Bring any changes and updates regarding the improvement in here\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_M4TooqSs2"
   },
   "source": [
    "## Part 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAEsMtIPLrO"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "peQ95zLuIpkk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
    "# idx is the index of the target bbox in the data\n",
    "# high-resolution image could be passed or could be load from data['file_name']\n",
    "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask\n",
    "#     to convert the segmentation annotations to binary masks\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "from detectron2.utils.visualizer import GenericMask\n",
    "def get_instance_sample(data, idx, img=None):\n",
    "    height, width = data['height'], data['width']\n",
    "    bbox = data['annotations'][idx]['bbox']\n",
    "    x1, y1 = int(bbox[0]), int(bbox[1])\n",
    "    x2, y2 = x1 + int(bbox[2]), y1 + int(bbox[3])\n",
    "    obj_img = cv2.imread(data['file_name'])[y1:y2,x1:x2,:]\n",
    "    obj_mask = GenericMask(data['annotations'][idx]['segmentation'], height, width).mask[y1:y2,x1:x2]\n",
    "    obj_img = cv2.resize(obj_img, (128, 128))\n",
    "    obj_mask = cv2.resize(obj_mask, (128, 128))\n",
    "    return obj_img, obj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "sxrc9X_pjzj-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# We have provided a template data loader for your segmentation training\n",
    "# You need to complete the __getitem__() function before running the code\n",
    "# You may also need to add data augmentation or normalization in here\n",
    "'''\n",
    "\n",
    "class PlaneDataset(Dataset):\n",
    "    def __init__(self, set_name, data_list):\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
    "        ])\n",
    "        self.set_name = set_name\n",
    "        self.data = data_list\n",
    "        self.instance_map = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            for j in range(len(d['annotations'])):\n",
    "                self.instance_map.append([i,j])\n",
    "\n",
    "    '''\n",
    "    # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
    "    # make sure to use the correct length for the final training\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.instance_map)\n",
    "\n",
    "    def numpy_to_tensor(self, img, mask):\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        img = torch.tensor(img, dtype=torch.float)\n",
    "        mask = torch.tensor(mask, dtype=torch.float)\n",
    "        return img, mask\n",
    "\n",
    "    '''\n",
    "    # Complete this part by using get_instance_sample function\n",
    "    # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
    "    # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
    "    # TODO: 5 lines\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx = self.instance_map[idx]\n",
    "        data = self.data[idx[0]]\n",
    "        \n",
    "        img, mask = get_instance_sample(data, idx[1])\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "def get_plane_dataset(set_name='train', batch_size=2):\n",
    "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
    "    dataset = PlaneDataset(set_name, my_data_list)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eH3NKaQQfc"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeqR3s3dSBPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
    "'''\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation=True):\n",
    "        super(conv, self).__init__()\n",
    "        if(activation):\n",
    "            self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "             nn.BatchNorm2d(out_ch),\n",
    "             nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# downsampling module equal to a conv module followed by a max-pool layer\n",
    "'''\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            conv(in_ch, out_ch),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# upsampling module equal to a upsample function followed by a conv module\n",
    "'''\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.up(x)\n",
    "        y = self.conv(y)\n",
    "        return y\n",
    "\n",
    "'''\n",
    "# the main model which you need to complete by using above modules.\n",
    "# you can also modify the above modules in order to improve your results.\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        self.input_conv = conv(3, 4)\n",
    "        self.down = down(4, 8)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        self.up = up(8, 4)\n",
    "        self.output_conv = conv(4, 1, False) # ReLu activation is removed to keep the logits for the loss function\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = self.input_conv(input)\n",
    "        y = self.down(y)\n",
    "        y = self.up(y)\n",
    "        output = self.output_conv(y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj86vD9QT_Z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaZuO4SKSBuF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# The following is a basic training procedure to train the network\n",
    "# You need to update the code to get the best performance\n",
    "# TODO: approx ? lines\n",
    "'''\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = MyModel() # initialize the model\n",
    "model = model.cuda() # move the model to GPU\n",
    "loader, _ = get_plane_dataset('train', batch_size) # initialize data_loader\n",
    "crit = nn.BCEWithLogitsLoss() # Define the loss function\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
    "\n",
    "# start the training procedure\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0\n",
    "  for (img, mask) in tqdm(loader):\n",
    "    img = torch.tensor(img, device=torch.device('cuda'), requires_grad = True)\n",
    "    mask = torch.tensor(mask, device=torch.device('cuda'), requires_grad = True)\n",
    "    pred = model(img)\n",
    "    loss = crit(pred, mask)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.cpu().data\n",
    "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
    "  torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
    "\n",
    "'''\n",
    "# Saving the final model\n",
    "'''\n",
    "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyez1fyQYw7"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeViryUSCL2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Before starting the evaluation, you need to set the model mode to eval\n",
    "# You may load the trained model again, in case if you want to continue your code later\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "batch_size = 8\n",
    "model = MyModel().cuda()\n",
    "model.load_state_dict(torch.load('{}/output/final_segmentation_model.pth'.format(BASE_DIR)))\n",
    "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
    "loader, dataset = get_plane_dataset('train', batch_size)\n",
    "\n",
    "total_iou = 0\n",
    "for (img, mask) in tqdm(loader):\n",
    "  with torch.no_grad():\n",
    "    img = img.cuda()\n",
    "    mask = mask.cuda()\n",
    "    mask = torch.unsqueeze(mask,1)\n",
    "    pred = model(img)\n",
    "\n",
    "    '''\n",
    "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
    "    '''\n",
    "\n",
    "\n",
    "print(\"\\n #images: {}, Mean IoU: {}\".format(_, _))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbmTj9JICiKz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize 3 sample outputs\n",
    "# TODO: approx 5 lines\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navoiGdrqaZT"
   },
   "source": [
    "## Part 3: Instance Segmentation\n",
    "\n",
    "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwk33DGBowP"
   },
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrZ8TG-Ot2J"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define a new function to obtain the prediction mask by passing a sample data\n",
    "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
    "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
    "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "\n",
    "def get_prediction_mask(data):\n",
    "\n",
    "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc7TSK6EBi9u"
   },
   "source": [
    "### Visualization and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T2YX8MBiGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPo_03up-g_f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
    "'''\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
    "    if(len(dots)==0):\n",
    "      return []\n",
    "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
    "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
    "    tmpdots = dots[inds]\n",
    "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
    "    inds = inds[1:] - inds[:-1]\n",
    "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
    "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
    "    return ' '.join([str(i) for i in runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv0rab2LJev-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# You need to upload the csv file on kaggle\n",
    "# The speed of your code in the previous parts highly affects the running time of this part\n",
    "'''\n",
    "\n",
    "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the training set\n",
    "'''\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for index in inds:\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index)\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the test set\n",
    "'''\n",
    "\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for j, index in enumerate(inds):\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index).double()\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
    "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nN4SduqrpI"
   },
   "source": [
    "## Part 4: Mask R-CNN\n",
    "\n",
    "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWf7drKNXYd"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1FDCcQN2LH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG5slAhQNjE7"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1pnKLMOcjS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ifeV1sNvtt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4K0Nz5OeKk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298QruFnNxyn"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcGwV5-9Oetp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_3wS2BFLLGp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
