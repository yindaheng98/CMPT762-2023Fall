{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "This is a template notebook for Assignment 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## Install dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-i4hmGYk1dL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some common libraries\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import some common pytorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tUA_j6AF1L5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that GPU is available for your notebook.\n",
    "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A_Di_fgL4HSv"
   },
   "outputs": [],
   "source": [
    "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
    "# TODO: approx 1 line\n",
    "BASE_DIR = '.'\n",
    "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## Part 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRV-KFJzlur4"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(os.path.join(data_dirs, \"train.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"val\" else train_annotations\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCH-2mWxDVVu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"val\", \"test\"]:\n",
    "    DatasetCatalog.register(\"data_detection_{}\".format(i), lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(\"data_detection_{}\".format(i)).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qNSdXCL_DVAz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "train_set = get_detection_data(\"train\")\n",
    "data = train_set[random.randrange(0, len(train_set))]\n",
    "img = cv2.imread(data[\"file_name\"])\n",
    "visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5)\n",
    "out = visualizer.draw_dataset_dict(data)\n",
    "save_path = os.path.join(BASE_DIR, \"output\", \"train_set.jpg\")\n",
    "cv2.imwrite(save_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"data_detection_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4rql8pNokE4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7d3KxiHO_0gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:26:48 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:26:51 d2.data.build]: Removed 0 images with no usable annotations. 159 images left.\n",
      "[10/21 03:26:51 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 6384         |             |              |\n",
      "|    total    | 6384         |             |              |             |              |\n",
      "[10/21 03:26:51 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[10/21 03:26:51 d2.data.build]: Using training sampler TrainingSampler\n",
      "[10/21 03:26:51 d2.data.common]: Serializing 159 elements to byte tensors and concatenating them all ...\n",
      "[10/21 03:26:51 d2.data.common]: Serialized dataset takes 13.26 MiB\n",
      "WARNING [10/21 03:26:51 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_f6e8b1.pkl: 243MB [00:05, 42.0MB/s]                                                                                                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:26:57 d2.engine.train_loop]: Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:27:28 d2.utils.events]:  eta: 0:04:24  iter: 19  total_loss: 3.617  loss_cls: 0.8876  loss_box_reg: 0.2714  loss_rpn_cls: 1.302  loss_rpn_loc: 0.2981  time: 1.2852  data_time: 1.1173  lr: 9.7405e-06  max_mem: 4471M\n",
      "[10/21 03:28:00 d2.utils.events]:  eta: 0:04:13  iter: 39  total_loss: 2.127  loss_cls: 0.8142  loss_box_reg: 0.2212  loss_rpn_cls: 0.4933  loss_rpn_loc: 0.4097  time: 1.4360  data_time: 1.2816  lr: 1.9731e-05  max_mem: 4471M\n",
      "[10/21 03:28:23 d2.utils.events]:  eta: 0:02:55  iter: 59  total_loss: 1.401  loss_cls: 0.3306  loss_box_reg: 0.1841  loss_rpn_cls: 0.3476  loss_rpn_loc: 0.3674  time: 1.3517  data_time: 0.7595  lr: 2.972e-05  max_mem: 4471M\n",
      "[10/21 03:28:46 d2.utils.events]:  eta: 0:04:02  iter: 79  total_loss: 1.079  loss_cls: 0.2046  loss_box_reg: 0.1929  loss_rpn_cls: 0.2627  loss_rpn_loc: 0.3645  time: 1.2919  data_time: 0.8394  lr: 3.9711e-05  max_mem: 4471M\n",
      "[10/21 03:29:04 d2.utils.events]:  eta: 0:03:10  iter: 99  total_loss: 1.182  loss_cls: 0.2956  loss_box_reg: 0.247  loss_rpn_cls: 0.2177  loss_rpn_loc: 0.3043  time: 1.2177  data_time: 0.4723  lr: 4.9701e-05  max_mem: 5645M\n",
      "[10/21 03:29:21 d2.utils.events]:  eta: 0:02:58  iter: 119  total_loss: 0.8701  loss_cls: 0.222  loss_box_reg: 0.1979  loss_rpn_cls: 0.2022  loss_rpn_loc: 0.2396  time: 1.1521  data_time: 0.5603  lr: 5.9691e-05  max_mem: 5645M\n",
      "[10/21 03:29:38 d2.utils.events]:  eta: 0:02:33  iter: 139  total_loss: 0.9048  loss_cls: 0.2068  loss_box_reg: 0.2337  loss_rpn_cls: 0.1837  loss_rpn_loc: 0.2658  time: 1.1058  data_time: 0.5769  lr: 6.9681e-05  max_mem: 5645M\n",
      "[10/21 03:30:03 d2.utils.events]:  eta: 0:02:39  iter: 159  total_loss: 0.7548  loss_cls: 0.145  loss_box_reg: 0.2038  loss_rpn_cls: 0.1899  loss_rpn_loc: 0.2009  time: 1.1247  data_time: 0.9239  lr: 7.9671e-05  max_mem: 5645M\n",
      "[10/21 03:30:26 d2.utils.events]:  eta: 0:02:30  iter: 179  total_loss: 0.9593  loss_cls: 0.1929  loss_box_reg: 0.2989  loss_rpn_cls: 0.1914  loss_rpn_loc: 0.2417  time: 1.1276  data_time: 0.5296  lr: 8.966e-05  max_mem: 6027M\n",
      "[10/21 03:30:42 d2.utils.events]:  eta: 0:02:15  iter: 199  total_loss: 0.9052  loss_cls: 0.1941  loss_box_reg: 0.2365  loss_rpn_cls: 0.1579  loss_rpn_loc: 0.2305  time: 1.0954  data_time: 0.5424  lr: 9.9651e-05  max_mem: 6027M\n",
      "[10/21 03:31:06 d2.utils.events]:  eta: 0:02:11  iter: 219  total_loss: 0.8296  loss_cls: 0.1947  loss_box_reg: 0.25  loss_rpn_cls: 0.1232  loss_rpn_loc: 0.2003  time: 1.1061  data_time: 0.9007  lr: 0.00010964  max_mem: 6027M\n",
      "[10/21 03:31:28 d2.utils.events]:  eta: 0:02:01  iter: 239  total_loss: 0.8926  loss_cls: 0.1725  loss_box_reg: 0.2201  loss_rpn_cls: 0.1783  loss_rpn_loc: 0.2591  time: 1.1039  data_time: 0.7738  lr: 0.00011963  max_mem: 6027M\n",
      "[10/21 03:31:45 d2.utils.events]:  eta: 0:01:46  iter: 259  total_loss: 0.8697  loss_cls: 0.1711  loss_box_reg: 0.2614  loss_rpn_cls: 0.1686  loss_rpn_loc: 0.2304  time: 1.0835  data_time: 0.5524  lr: 0.00012962  max_mem: 6027M\n",
      "[10/21 03:32:02 d2.utils.events]:  eta: 0:01:28  iter: 279  total_loss: 0.8347  loss_cls: 0.1562  loss_box_reg: 0.2565  loss_rpn_cls: 0.1326  loss_rpn_loc: 0.2084  time: 1.0681  data_time: 0.5912  lr: 0.00013961  max_mem: 6027M\n",
      "[10/21 03:32:19 d2.utils.events]:  eta: 0:01:14  iter: 299  total_loss: 0.8939  loss_cls: 0.136  loss_box_reg: 0.2339  loss_rpn_cls: 0.1346  loss_rpn_loc: 0.2604  time: 1.0540  data_time: 0.5638  lr: 0.0001496  max_mem: 6027M\n",
      "[10/21 03:32:41 d2.utils.events]:  eta: 0:01:09  iter: 319  total_loss: 0.9334  loss_cls: 0.2281  loss_box_reg: 0.3076  loss_rpn_cls: 0.1359  loss_rpn_loc: 0.2074  time: 1.0543  data_time: 0.8104  lr: 0.00015959  max_mem: 6027M\n",
      "[10/21 03:32:56 d2.utils.events]:  eta: 0:00:57  iter: 339  total_loss: 0.818  loss_cls: 0.1547  loss_box_reg: 0.2201  loss_rpn_cls: 0.1598  loss_rpn_loc: 0.2821  time: 1.0384  data_time: 0.4968  lr: 0.00016958  max_mem: 6027M\n",
      "[10/21 03:33:25 d2.utils.events]:  eta: 0:00:49  iter: 359  total_loss: 0.7925  loss_cls: 0.1766  loss_box_reg: 0.3051  loss_rpn_cls: 0.1121  loss_rpn_loc: 0.1976  time: 1.0591  data_time: 0.7504  lr: 0.00017957  max_mem: 7190M\n",
      "[10/21 03:33:45 d2.utils.events]:  eta: 0:00:37  iter: 379  total_loss: 0.6713  loss_cls: 0.1447  loss_box_reg: 0.2462  loss_rpn_cls: 0.1022  loss_rpn_loc: 0.2012  time: 1.0580  data_time: 0.7657  lr: 0.00018956  max_mem: 7190M\n",
      "[10/21 03:34:09 d2.utils.events]:  eta: 0:00:31  iter: 399  total_loss: 0.7499  loss_cls: 0.1342  loss_box_reg: 0.2574  loss_rpn_cls: 0.1471  loss_rpn_loc: 0.2273  time: 1.0640  data_time: 0.9187  lr: 0.00019955  max_mem: 7190M\n",
      "[10/21 03:34:25 d2.utils.events]:  eta: 0:00:25  iter: 419  total_loss: 0.7457  loss_cls: 0.1268  loss_box_reg: 0.2186  loss_rpn_cls: 0.1298  loss_rpn_loc: 0.2021  time: 1.0506  data_time: 0.5395  lr: 0.00020954  max_mem: 7190M\n",
      "[10/21 03:34:44 d2.utils.events]:  eta: 0:00:18  iter: 439  total_loss: 0.7234  loss_cls: 0.1441  loss_box_reg: 0.2537  loss_rpn_cls: 0.1163  loss_rpn_loc: 0.2364  time: 1.0465  data_time: 0.6740  lr: 0.00021953  max_mem: 7190M\n",
      "[10/21 03:35:05 d2.utils.events]:  eta: 0:00:13  iter: 459  total_loss: 0.865  loss_cls: 0.1768  loss_box_reg: 0.2724  loss_rpn_cls: 0.1211  loss_rpn_loc: 0.2025  time: 1.0471  data_time: 0.7516  lr: 0.00022952  max_mem: 7190M\n",
      "[10/21 03:35:29 d2.utils.events]:  eta: 0:00:07  iter: 479  total_loss: 0.751  loss_cls: 0.1467  loss_box_reg: 0.2871  loss_rpn_cls: 0.1126  loss_rpn_loc: 0.1852  time: 1.0524  data_time: 0.8524  lr: 0.00023951  max_mem: 7190M\n",
      "[10/21 03:35:47 d2.utils.events]:  eta: 0:00:00  iter: 499  total_loss: 0.7361  loss_cls: 0.1225  loss_box_reg: 0.2173  loss_rpn_cls: 0.1206  loss_rpn_loc: 0.2272  time: 1.0463  data_time: 0.6095  lr: 0.0002495  max_mem: 7190M\n",
      "[10/21 03:35:47 d2.engine.hooks]: Overall training speed: 498 iterations in 0:08:41 (1.0463 s / it)\n",
      "[10/21 03:35:47 d2.engine.hooks]: Total training time: 0:08:42 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRVEiICco3SV"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VYCIXdMZvDYL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_hRCf86KGi5v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for i in range(3):\n",
    "    idx = random.randrange(0, len(test_set))\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D0wRdlcKo6BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/21 03:35:55 d2.evaluation.coco_evaluation]: Trying to convert 'data_detection_val' to COCO format ...\n",
      "WARNING [10/21 03:35:55 d2.data.datasets.coco]: Using previously cached COCO format annotations at './output/data_detection_val_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "[10/21 03:35:56 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 1596         |             |              |\n",
      "|    total    | 1596         |             |              |             |              |\n",
      "[10/21 03:35:56 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[10/21 03:35:56 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...\n",
      "[10/21 03:35:56 d2.data.common]: Serialized dataset takes 2.21 MiB\n",
      "[10/21 03:35:56 d2.evaluation.evaluator]: Start inference on 40 batches\n",
      "[10/21 03:36:01 d2.evaluation.evaluator]: Inference done 11/40. Dataloading: 0.0353 s/iter. Inference: 0.0489 s/iter. Eval: 0.0004 s/iter. Total: 0.0846 s/iter. ETA=0:00:02\n",
      "[10/21 03:36:06 d2.evaluation.evaluator]: Inference done 26/40. Dataloading: 0.2128 s/iter. Inference: 0.0496 s/iter. Eval: 0.0003 s/iter. Total: 0.2629 s/iter. ETA=0:00:03\n",
      "[10/21 03:36:11 d2.evaluation.evaluator]: Inference done 35/40. Dataloading: 0.3038 s/iter. Inference: 0.0543 s/iter. Eval: 0.0003 s/iter. Total: 0.3585 s/iter. ETA=0:00:01\n",
      "[10/21 03:36:12 d2.evaluation.evaluator]: Total inference time: 0:00:11.851672 (0.338619 s / iter per device, on 1 devices)\n",
      "[10/21 03:36:12 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:01 (0.054815 s / iter per device, on 1 devices)\n",
      "[10/21 03:36:12 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[10/21 03:36:12 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[10/21 03:36:12 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[10/21 03:36:12 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[10/21 03:36:12 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[10/21 03:36:12 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[10/21 03:36:12 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.487\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.248\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.211\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.134\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.297\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.481\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.733\n",
      "[10/21 03:36:12 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 26.019 | 48.737 | 24.809 | 21.112 | 41.955 | 56.584 |\n",
      "[10/21 03:36:12 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category    | AP   | category    | AP     | category    | AP   |\n",
      "|:------------|:-----|:------------|:-------|:------------|:-----|\n",
      "| not plane 1 | nan  | not plane 2 | nan    | not plane 3 | nan  |\n",
      "| not plane 4 | nan  | plane       | 26.019 |             |      |\n",
      "OrderedDict([('bbox', {'AP': 26.0189534751326, 'AP50': 48.73732583442221, 'AP75': 24.80885909822931, 'APs': 21.112011025800374, 'APm': 41.954961234052405, 'APl': 56.58446671448103, 'AP-not plane 1': nan, 'AP-not plane 2': nan, 'AP-not plane 3': nan, 'AP-not plane 4': nan, 'AP-plane': 26.0189534751326})])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"data_detection_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwYvbwcjpKBk"
   },
   "source": [
    "### Improvements\n",
    "\n",
    "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xdR6KbCZpOlk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Bring any changes and updates regarding the improvement in here\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Bring any changes and updates regarding the improvement in here\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_M4TooqSs2"
   },
   "source": [
    "## Part 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAEsMtIPLrO"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "peQ95zLuIpkk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
    "# idx is the index of the target bbox in the data\n",
    "# high-resolution image could be passed or could be load from data['file_name']\n",
    "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask\n",
    "#     to convert the segmentation annotations to binary masks\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "from detectron2.utils.visualizer import GenericMask\n",
    "def get_instance_sample(data, idx, img=None):\n",
    "    height, width = data['height'], data['width']\n",
    "    bbox = data['annotations'][idx]['bbox']\n",
    "    x1, y1 = int(bbox[0]), int(bbox[1])\n",
    "    x2, y2 = x1 + int(bbox[2]), y1 + int(bbox[3])\n",
    "    obj_img = cv2.imread(data['file_name'])[y1:y2,x1:x2,:]\n",
    "    obj_mask = GenericMask(data['annotations'][idx]['segmentation'], height, width).mask[y1:y2,x1:x2]\n",
    "    obj_img = cv2.resize(obj_img, (128, 128))\n",
    "    obj_mask = cv2.resize(obj_mask, (128, 128))\n",
    "    return obj_img, obj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sxrc9X_pjzj-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# We have provided a template data loader for your segmentation training\n",
    "# You need to complete the __getitem__() function before running the code\n",
    "# You may also need to add data augmentation or normalization in here\n",
    "'''\n",
    "\n",
    "class PlaneDataset(Dataset):\n",
    "    def __init__(self, set_name, data_list):\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
    "        ])\n",
    "        self.set_name = set_name\n",
    "        self.data = data_list\n",
    "        self.instance_map = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            for j in range(len(d['annotations'])):\n",
    "                self.instance_map.append([i,j])\n",
    "\n",
    "    '''\n",
    "    # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
    "    # make sure to use the correct length for the final training\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.instance_map)\n",
    "\n",
    "    def numpy_to_tensor(self, img, mask):\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        img = torch.tensor(img, dtype=torch.float)\n",
    "        mask = torch.tensor(mask, dtype=torch.float)\n",
    "        return img, mask\n",
    "\n",
    "    '''\n",
    "    # Complete this part by using get_instance_sample function\n",
    "    # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
    "    # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
    "    # TODO: 5 lines\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx = self.instance_map[idx]\n",
    "        data = self.data[idx[0]]\n",
    "        \n",
    "        img, mask = get_instance_sample(data, idx[1])\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "def get_plane_dataset(set_name='train', batch_size=2):\n",
    "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
    "    dataset = PlaneDataset(set_name, my_data_list)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eH3NKaQQfc"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PeqR3s3dSBPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
    "'''\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation=True):\n",
    "        super(conv, self).__init__()\n",
    "        if(activation):\n",
    "            self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "             nn.BatchNorm2d(out_ch),\n",
    "             nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "             nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# downsampling module equal to a conv module followed by a max-pool layer\n",
    "'''\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            conv(in_ch, out_ch),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "# upsampling module equal to a upsample function followed by a conv module\n",
    "'''\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.up(x)\n",
    "        y = self.conv(y)\n",
    "        return y\n",
    "\n",
    "'''\n",
    "# the main model which you need to complete by using above modules.\n",
    "# you can also modify the above modules in order to improve your results.\n",
    "'''\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        self.input_conv = conv(3, 4)\n",
    "        self.down = down(4, 8)\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        self.up = up(8, 4)\n",
    "        self.output_conv = conv(4, 1, False) # ReLu activation is removed to keep the logits for the loss function\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        y = self.input_conv(input)\n",
    "        y = self.down(y)\n",
    "        y = self.up(y)\n",
    "        output = self.output_conv(y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj86vD9QT_Z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MaZuO4SKSBuF"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005372285842895508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 16,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1596,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5563b0c52d480f9818d8b37fc8fe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4, 3, 3, 3], expected input[4, 128, 128, 3] to have 3 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_853/3321007099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_853/601757523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_853/601757523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4, 3, 3, 3], expected input[4, 128, 128, 3] to have 3 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# The following is a basic training procedure to train the network\n",
    "# You need to update the code to get the best performance\n",
    "# TODO: approx ? lines\n",
    "'''\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = MyModel() # initialize the model\n",
    "model = model.cuda() # move the model to GPU\n",
    "loader, _ = get_plane_dataset('train', batch_size) # initialize data_loader\n",
    "crit = nn.BCEWithLogitsLoss() # Define the loss function\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
    "\n",
    "# start the training procedure\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0\n",
    "  for (img, mask) in tqdm(loader):\n",
    "    img = torch.tensor(img.type(torch.float), device=torch.device('cuda'), requires_grad = True)\n",
    "    mask = torch.tensor(mask.type(torch.float), device=torch.device('cuda'), requires_grad = True)\n",
    "    pred = model(img)\n",
    "    loss = crit(pred, mask)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.cpu().data\n",
    "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
    "  torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
    "\n",
    "'''\n",
    "# Saving the final model\n",
    "'''\n",
    "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyez1fyQYw7"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeViryUSCL2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Before starting the evaluation, you need to set the model mode to eval\n",
    "# You may load the trained model again, in case if you want to continue your code later\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "batch_size = 8\n",
    "model = MyModel().cuda()\n",
    "model.load_state_dict(torch.load('{}/output/final_segmentation_model.pth'.format(BASE_DIR)))\n",
    "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
    "loader, dataset = get_plane_dataset('train', batch_size)\n",
    "\n",
    "total_iou = 0\n",
    "for (img, mask) in tqdm(loader):\n",
    "  with torch.no_grad():\n",
    "    img = img.cuda()\n",
    "    mask = mask.cuda()\n",
    "    mask = torch.unsqueeze(mask,1)\n",
    "    pred = model(img)\n",
    "\n",
    "    '''\n",
    "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
    "    '''\n",
    "\n",
    "\n",
    "print(\"\\n #images: {}, Mean IoU: {}\".format(_, _))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbmTj9JICiKz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize 3 sample outputs\n",
    "# TODO: approx 5 lines\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navoiGdrqaZT"
   },
   "source": [
    "## Part 3: Instance Segmentation\n",
    "\n",
    "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwk33DGBowP"
   },
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrZ8TG-Ot2J"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define a new function to obtain the prediction mask by passing a sample data\n",
    "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
    "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
    "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "\n",
    "def get_prediction_mask(data):\n",
    "\n",
    "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc7TSK6EBi9u"
   },
   "source": [
    "### Visualization and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T2YX8MBiGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPo_03up-g_f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
    "'''\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
    "    if(len(dots)==0):\n",
    "      return []\n",
    "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
    "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
    "    tmpdots = dots[inds]\n",
    "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
    "    inds = inds[1:] - inds[:-1]\n",
    "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
    "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
    "    return ' '.join([str(i) for i in runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv0rab2LJev-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# You need to upload the csv file on kaggle\n",
    "# The speed of your code in the previous parts highly affects the running time of this part\n",
    "'''\n",
    "\n",
    "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the training set\n",
    "'''\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for index in inds:\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index)\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the test set\n",
    "'''\n",
    "\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for j, index in enumerate(inds):\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index).double()\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
    "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nN4SduqrpI"
   },
   "source": [
    "## Part 4: Mask R-CNN\n",
    "\n",
    "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWf7drKNXYd"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1FDCcQN2LH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG5slAhQNjE7"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1pnKLMOcjS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ifeV1sNvtt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4K0Nz5OeKk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298QruFnNxyn"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcGwV5-9Oetp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_3wS2BFLLGp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
