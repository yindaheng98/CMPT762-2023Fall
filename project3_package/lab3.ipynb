{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "This is a template notebook for Assignment 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## Install dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b-i4hmGYk1dL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some common libraries\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from sklearn.metrics import jaccard_score\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# import some common pytorch utilities\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tUA_j6AF1L5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that GPU is available for your notebook.\n",
    "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A_Di_fgL4HSv"
   },
   "outputs": [],
   "source": [
    "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
    "# TODO: approx 1 line\n",
    "BASE_DIR = '.'\n",
    "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## Part 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRV-KFJzlur4"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name, datapath=\"data/train.json\"):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(datapath) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"val\" else (data if set_name == \"all\" else train_annotations)\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xCH-2mWxDVVu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='data_detection_all_ori',\n",
       "          thing_classes=['not plane 1',\n",
       "                         'not plane 2',\n",
       "                         'not plane 3',\n",
       "                         'not plane 4',\n",
       "                         'plane'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"val\", \"all\", \"test\"]:\n",
    "    DatasetCatalog.register(\"data_detection_{}\".format(i), lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(\"data_detection_{}\".format(i)).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])\n",
    "\n",
    "DatasetCatalog.register(\"data_detection_all_ori\", lambda i=i: get_detection_data(\"all\", datapath=\"data/train.json\"))\n",
    "MetadataCatalog.get(\"data_detection_all_ori\").set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qNSdXCL_DVAz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "train_set = get_detection_data(\"train\")\n",
    "data = train_set[random.randrange(0, len(train_set))]\n",
    "img = cv2.imread(data[\"file_name\"])\n",
    "visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5)\n",
    "out = visualizer.draw_dataset_dict(data)\n",
    "save_path = os.path.join(BASE_DIR, \"output\", \"train_set.jpg\")\n",
    "cv2.imwrite(save_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"data_detection_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 1200\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4rql8pNokE4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7d3KxiHO_0gb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 05:23:16 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=81, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 05:23:17 d2.data.build]: Removed 0 images with no usable annotations. 159 images left.\n",
      "[11/11 05:23:17 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 6384         |             |              |\n",
      "|    total    | 6384         |             |              |             |              |\n",
      "[11/11 05:23:17 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[11/11 05:23:17 d2.data.build]: Using training sampler TrainingSampler\n",
      "[11/11 05:23:17 d2.data.common]: Serializing 159 elements to byte tensors and concatenating them all ...\n",
      "[11/11 05:23:17 d2.data.common]: Serialized dataset takes 13.26 MiB\n",
      "WARNING [11/11 05:23:18 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "[11/11 05:23:18 d2.engine.train_loop]: Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 05:23:38 d2.utils.events]:  eta: 0:05:26  iter: 19  total_loss: 3.688  loss_cls: 1.051  loss_box_reg: 0.2366  loss_rpn_cls: 1.712  loss_rpn_loc: 0.5286  time: 0.9299  data_time: 0.7127  lr: 4.9953e-06  max_mem: 3743M\n",
      "[11/11 05:24:07 d2.utils.events]:  eta: 0:08:17  iter: 39  total_loss: 2.047  loss_cls: 0.5626  loss_box_reg: 0.1987  loss_rpn_cls: 0.6582  loss_rpn_loc: 0.3284  time: 1.1856  data_time: 1.0675  lr: 9.9902e-06  max_mem: 4730M\n",
      "[11/11 05:24:32 d2.utils.events]:  eta: 0:05:49  iter: 59  total_loss: 1.413  loss_cls: 0.5444  loss_box_reg: 0.2317  loss_rpn_cls: 0.1745  loss_rpn_loc: 0.2207  time: 1.2062  data_time: 0.9572  lr: 1.4985e-05  max_mem: 4730M\n",
      "[11/11 05:24:53 d2.utils.events]:  eta: 0:06:01  iter: 79  total_loss: 1.709  loss_cls: 0.4936  loss_box_reg: 0.3135  loss_rpn_cls: 0.2666  loss_rpn_loc: 0.3798  time: 1.1735  data_time: 0.7909  lr: 1.998e-05  max_mem: 4730M\n",
      "[11/11 05:25:11 d2.utils.events]:  eta: 0:05:37  iter: 99  total_loss: 1.223  loss_cls: 0.2307  loss_box_reg: 0.1491  loss_rpn_cls: 0.2854  loss_rpn_loc: 0.2982  time: 1.1134  data_time: 0.6049  lr: 2.4975e-05  max_mem: 5223M\n",
      "[11/11 05:25:31 d2.utils.events]:  eta: 0:05:12  iter: 119  total_loss: 1.085  loss_cls: 0.2819  loss_box_reg: 0.2266  loss_rpn_cls: 0.2229  loss_rpn_loc: 0.3  time: 1.0974  data_time: 0.5621  lr: 2.997e-05  max_mem: 5852M\n",
      "[11/11 05:25:46 d2.utils.events]:  eta: 0:05:06  iter: 139  total_loss: 0.9172  loss_cls: 0.1719  loss_box_reg: 0.195  loss_rpn_cls: 0.196  loss_rpn_loc: 0.292  time: 1.0480  data_time: 0.5227  lr: 3.4965e-05  max_mem: 5852M\n",
      "[11/11 05:26:03 d2.utils.events]:  eta: 0:04:56  iter: 159  total_loss: 1.068  loss_cls: 0.198  loss_box_reg: 0.2286  loss_rpn_cls: 0.2147  loss_rpn_loc: 0.2869  time: 1.0224  data_time: 0.5946  lr: 3.996e-05  max_mem: 5852M\n",
      "[11/11 05:26:16 d2.utils.events]:  eta: 0:04:50  iter: 179  total_loss: 0.9729  loss_cls: 0.2053  loss_box_reg: 0.2173  loss_rpn_cls: 0.1543  loss_rpn_loc: 0.2275  time: 0.9781  data_time: 0.3747  lr: 4.4955e-05  max_mem: 5852M\n",
      "[11/11 05:26:39 d2.utils.events]:  eta: 0:04:53  iter: 199  total_loss: 0.9276  loss_cls: 0.1611  loss_box_reg: 0.1861  loss_rpn_cls: 0.2181  loss_rpn_loc: 0.2527  time: 0.9974  data_time: 0.9223  lr: 4.995e-05  max_mem: 5852M\n",
      "[11/11 05:27:06 d2.utils.events]:  eta: 0:04:43  iter: 219  total_loss: 0.8843  loss_cls: 0.1587  loss_box_reg: 0.2666  loss_rpn_cls: 0.1785  loss_rpn_loc: 0.2729  time: 1.0278  data_time: 0.7541  lr: 5.4945e-05  max_mem: 6497M\n",
      "[11/11 05:27:21 d2.utils.events]:  eta: 0:04:33  iter: 239  total_loss: 1.046  loss_cls: 0.2717  loss_box_reg: 0.2729  loss_rpn_cls: 0.207  loss_rpn_loc: 0.2588  time: 1.0038  data_time: 0.4943  lr: 5.994e-05  max_mem: 6497M\n",
      "[11/11 05:27:37 d2.utils.events]:  eta: 0:04:29  iter: 259  total_loss: 0.8416  loss_cls: 0.1723  loss_box_reg: 0.292  loss_rpn_cls: 0.1649  loss_rpn_loc: 0.1602  time: 0.9879  data_time: 0.5519  lr: 6.4935e-05  max_mem: 6497M\n",
      "[11/11 05:27:54 d2.utils.events]:  eta: 0:04:27  iter: 279  total_loss: 0.8535  loss_cls: 0.1557  loss_box_reg: 0.2387  loss_rpn_cls: 0.1785  loss_rpn_loc: 0.2373  time: 0.9787  data_time: 0.6072  lr: 6.993e-05  max_mem: 6497M\n",
      "[11/11 05:28:17 d2.utils.events]:  eta: 0:04:17  iter: 299  total_loss: 0.9161  loss_cls: 0.1733  loss_box_reg: 0.2049  loss_rpn_cls: 0.173  loss_rpn_loc: 0.2682  time: 0.9910  data_time: 0.8887  lr: 7.4925e-05  max_mem: 6497M\n",
      "[11/11 05:28:40 d2.utils.events]:  eta: 0:04:15  iter: 319  total_loss: 0.6752  loss_cls: 0.1378  loss_box_reg: 0.1942  loss_rpn_cls: 0.1615  loss_rpn_loc: 0.1969  time: 0.9997  data_time: 0.8136  lr: 7.992e-05  max_mem: 6497M\n",
      "[11/11 05:28:56 d2.utils.events]:  eta: 0:04:13  iter: 339  total_loss: 0.7977  loss_cls: 0.1793  loss_box_reg: 0.2698  loss_rpn_cls: 0.1415  loss_rpn_loc: 0.2137  time: 0.9888  data_time: 0.5609  lr: 8.4915e-05  max_mem: 6497M\n",
      "[11/11 05:29:19 d2.utils.events]:  eta: 0:04:12  iter: 359  total_loss: 1.018  loss_cls: 0.2038  loss_box_reg: 0.31  loss_rpn_cls: 0.1886  loss_rpn_loc: 0.2327  time: 0.9984  data_time: 0.8758  lr: 8.991e-05  max_mem: 6497M\n",
      "[11/11 05:29:39 d2.utils.events]:  eta: 0:04:06  iter: 379  total_loss: 0.867  loss_cls: 0.172  loss_box_reg: 0.2847  loss_rpn_cls: 0.144  loss_rpn_loc: 0.2037  time: 0.9984  data_time: 0.7172  lr: 9.4905e-05  max_mem: 6497M\n",
      "[11/11 05:29:53 d2.utils.events]:  eta: 0:03:58  iter: 399  total_loss: 0.756  loss_cls: 0.1599  loss_box_reg: 0.1993  loss_rpn_cls: 0.1579  loss_rpn_loc: 0.2156  time: 0.9831  data_time: 0.4560  lr: 9.99e-05  max_mem: 6497M\n",
      "[11/11 05:30:14 d2.utils.events]:  eta: 0:03:48  iter: 419  total_loss: 0.7765  loss_cls: 0.1588  loss_box_reg: 0.2627  loss_rpn_cls: 0.1518  loss_rpn_loc: 0.2058  time: 0.9861  data_time: 0.7896  lr: 0.0001049  max_mem: 6497M\n",
      "[11/11 05:30:35 d2.utils.events]:  eta: 0:03:43  iter: 439  total_loss: 0.7103  loss_cls: 0.1441  loss_box_reg: 0.2451  loss_rpn_cls: 0.1262  loss_rpn_loc: 0.1231  time: 0.9873  data_time: 0.7364  lr: 0.00010989  max_mem: 6497M\n",
      "[11/11 05:30:54 d2.utils.events]:  eta: 0:03:37  iter: 459  total_loss: 0.8644  loss_cls: 0.1831  loss_box_reg: 0.2521  loss_rpn_cls: 0.1366  loss_rpn_loc: 0.2591  time: 0.9870  data_time: 0.7127  lr: 0.00011489  max_mem: 6497M\n",
      "[11/11 05:31:23 d2.utils.events]:  eta: 0:03:31  iter: 479  total_loss: 1  loss_cls: 0.2177  loss_box_reg: 0.2942  loss_rpn_cls: 0.1508  loss_rpn_loc: 0.262  time: 1.0062  data_time: 0.7379  lr: 0.00011988  max_mem: 6923M\n",
      "[11/11 05:31:50 d2.utils.events]:  eta: 0:03:28  iter: 499  total_loss: 0.8222  loss_cls: 0.1714  loss_box_reg: 0.2811  loss_rpn_cls: 0.1186  loss_rpn_loc: 0.1841  time: 1.0192  data_time: 0.9515  lr: 0.00012488  max_mem: 6923M\n",
      "[11/11 05:32:12 d2.utils.events]:  eta: 0:03:24  iter: 519  total_loss: 0.894  loss_cls: 0.1635  loss_box_reg: 0.2901  loss_rpn_cls: 0.1244  loss_rpn_loc: 0.2677  time: 1.0227  data_time: 0.8314  lr: 0.00012987  max_mem: 6923M\n",
      "[11/11 05:32:29 d2.utils.events]:  eta: 0:03:18  iter: 539  total_loss: 0.8364  loss_cls: 0.1707  loss_box_reg: 0.2244  loss_rpn_cls: 0.1353  loss_rpn_loc: 0.2317  time: 1.0158  data_time: 0.5797  lr: 0.00013487  max_mem: 6923M\n",
      "[11/11 05:32:47 d2.utils.events]:  eta: 0:03:10  iter: 559  total_loss: 0.8462  loss_cls: 0.2075  loss_box_reg: 0.2537  loss_rpn_cls: 0.1445  loss_rpn_loc: 0.2165  time: 1.0123  data_time: 0.6443  lr: 0.00013986  max_mem: 6923M\n",
      "[11/11 05:33:02 d2.utils.events]:  eta: 0:03:06  iter: 579  total_loss: 0.7274  loss_cls: 0.1529  loss_box_reg: 0.2755  loss_rpn_cls: 0.1018  loss_rpn_loc: 0.1853  time: 1.0036  data_time: 0.4012  lr: 0.00014486  max_mem: 6923M\n",
      "[11/11 05:33:28 d2.utils.events]:  eta: 0:03:00  iter: 599  total_loss: 0.6825  loss_cls: 0.1313  loss_box_reg: 0.2825  loss_rpn_cls: 0.08852  loss_rpn_loc: 0.143  time: 1.0133  data_time: 0.9788  lr: 0.00014985  max_mem: 6923M\n",
      "[11/11 05:33:44 d2.utils.events]:  eta: 0:02:53  iter: 619  total_loss: 0.8246  loss_cls: 0.1614  loss_box_reg: 0.2441  loss_rpn_cls: 0.1233  loss_rpn_loc: 0.2162  time: 1.0059  data_time: 0.5446  lr: 0.00015485  max_mem: 6923M\n",
      "[11/11 05:34:05 d2.utils.events]:  eta: 0:02:46  iter: 639  total_loss: 0.7426  loss_cls: 0.1596  loss_box_reg: 0.252  loss_rpn_cls: 0.1215  loss_rpn_loc: 0.218  time: 1.0080  data_time: 0.7974  lr: 0.00015984  max_mem: 6923M\n",
      "[11/11 05:34:21 d2.utils.events]:  eta: 0:02:40  iter: 659  total_loss: 0.7967  loss_cls: 0.104  loss_box_reg: 0.2158  loss_rpn_cls: 0.1231  loss_rpn_loc: 0.2616  time: 1.0011  data_time: 0.5181  lr: 0.00016484  max_mem: 6923M\n",
      "[11/11 05:34:39 d2.utils.events]:  eta: 0:02:32  iter: 679  total_loss: 0.7242  loss_cls: 0.1896  loss_box_reg: 0.305  loss_rpn_cls: 0.1048  loss_rpn_loc: 0.1887  time: 0.9983  data_time: 0.6318  lr: 0.00016983  max_mem: 6923M\n",
      "[11/11 05:35:04 d2.utils.events]:  eta: 0:02:27  iter: 699  total_loss: 0.66  loss_cls: 0.1271  loss_box_reg: 0.2662  loss_rpn_cls: 0.09125  loss_rpn_loc: 0.1719  time: 1.0044  data_time: 0.8359  lr: 0.00017483  max_mem: 6923M\n",
      "[11/11 05:35:25 d2.utils.events]:  eta: 0:02:21  iter: 719  total_loss: 0.8108  loss_cls: 0.1789  loss_box_reg: 0.2734  loss_rpn_cls: 0.136  loss_rpn_loc: 0.2211  time: 1.0057  data_time: 0.6933  lr: 0.00017982  max_mem: 6923M\n",
      "[11/11 05:35:38 d2.utils.events]:  eta: 0:02:14  iter: 739  total_loss: 0.7412  loss_cls: 0.1387  loss_box_reg: 0.2592  loss_rpn_cls: 0.1144  loss_rpn_loc: 0.2011  time: 0.9969  data_time: 0.4319  lr: 0.00018482  max_mem: 6923M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 05:35:59 d2.utils.events]:  eta: 0:02:10  iter: 759  total_loss: 0.7079  loss_cls: 0.1513  loss_box_reg: 0.253  loss_rpn_cls: 0.09299  loss_rpn_loc: 0.1706  time: 0.9979  data_time: 0.7713  lr: 0.00018981  max_mem: 6923M\n",
      "[11/11 05:36:15 d2.utils.events]:  eta: 0:02:03  iter: 779  total_loss: 0.6337  loss_cls: 0.1219  loss_box_reg: 0.2003  loss_rpn_cls: 0.09463  loss_rpn_loc: 0.1905  time: 0.9923  data_time: 0.4930  lr: 0.00019481  max_mem: 6923M\n",
      "[11/11 05:36:36 d2.utils.events]:  eta: 0:01:58  iter: 799  total_loss: 0.7787  loss_cls: 0.172  loss_box_reg: 0.3014  loss_rpn_cls: 0.1188  loss_rpn_loc: 0.217  time: 0.9942  data_time: 0.7170  lr: 0.0001998  max_mem: 6923M\n",
      "[11/11 05:36:51 d2.utils.events]:  eta: 0:01:52  iter: 819  total_loss: 0.676  loss_cls: 0.119  loss_box_reg: 0.2112  loss_rpn_cls: 0.07871  loss_rpn_loc: 0.1703  time: 0.9885  data_time: 0.5109  lr: 0.0002048  max_mem: 6923M\n",
      "[11/11 05:37:08 d2.utils.events]:  eta: 0:01:46  iter: 839  total_loss: 0.6147  loss_cls: 0.1105  loss_box_reg: 0.2367  loss_rpn_cls: 0.105  loss_rpn_loc: 0.1987  time: 0.9846  data_time: 0.5450  lr: 0.00020979  max_mem: 6923M\n",
      "[11/11 05:37:24 d2.utils.events]:  eta: 0:01:40  iter: 859  total_loss: 0.6848  loss_cls: 0.1487  loss_box_reg: 0.2227  loss_rpn_cls: 0.0706  loss_rpn_loc: 0.2268  time: 0.9808  data_time: 0.5739  lr: 0.00021479  max_mem: 6923M\n",
      "[11/11 05:37:45 d2.utils.events]:  eta: 0:01:34  iter: 879  total_loss: 0.7917  loss_cls: 0.1801  loss_box_reg: 0.2561  loss_rpn_cls: 0.1009  loss_rpn_loc: 0.1929  time: 0.9823  data_time: 0.7776  lr: 0.00021978  max_mem: 6923M\n",
      "[11/11 05:38:08 d2.utils.events]:  eta: 0:01:28  iter: 899  total_loss: 0.7617  loss_cls: 0.1414  loss_box_reg: 0.2318  loss_rpn_cls: 0.09675  loss_rpn_loc: 0.2053  time: 0.9861  data_time: 0.8631  lr: 0.00022478  max_mem: 6923M\n",
      "[11/11 05:38:24 d2.utils.events]:  eta: 0:01:22  iter: 919  total_loss: 0.6118  loss_cls: 0.1321  loss_box_reg: 0.1796  loss_rpn_cls: 0.0811  loss_rpn_loc: 0.1614  time: 0.9814  data_time: 0.5177  lr: 0.00022977  max_mem: 6923M\n",
      "[11/11 05:38:40 d2.utils.events]:  eta: 0:01:16  iter: 939  total_loss: 0.6405  loss_cls: 0.1319  loss_box_reg: 0.2537  loss_rpn_cls: 0.0825  loss_rpn_loc: 0.1961  time: 0.9777  data_time: 0.4928  lr: 0.00023477  max_mem: 6923M\n",
      "[11/11 05:38:56 d2.utils.events]:  eta: 0:01:10  iter: 959  total_loss: 0.6454  loss_cls: 0.1515  loss_box_reg: 0.2896  loss_rpn_cls: 0.05862  loss_rpn_loc: 0.1609  time: 0.9740  data_time: 0.5378  lr: 0.00023976  max_mem: 6923M\n",
      "[11/11 05:39:12 d2.utils.events]:  eta: 0:01:04  iter: 979  total_loss: 0.506  loss_cls: 0.1063  loss_box_reg: 0.2337  loss_rpn_cls: 0.05055  loss_rpn_loc: 0.1364  time: 0.9701  data_time: 0.5116  lr: 0.00024476  max_mem: 6923M\n",
      "[11/11 05:39:29 d2.utils.events]:  eta: 0:00:58  iter: 999  total_loss: 0.7445  loss_cls: 0.1522  loss_box_reg: 0.2581  loss_rpn_cls: 0.1082  loss_rpn_loc: 0.2183  time: 0.9685  data_time: 0.6512  lr: 0.00024975  max_mem: 6923M\n",
      "[11/11 05:39:47 d2.utils.events]:  eta: 0:00:53  iter: 1019  total_loss: 0.6326  loss_cls: 0.1346  loss_box_reg: 0.2237  loss_rpn_cls: 0.08382  loss_rpn_loc: 0.1684  time: 0.9664  data_time: 0.5588  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:40:03 d2.utils.events]:  eta: 0:00:47  iter: 1039  total_loss: 0.7501  loss_cls: 0.1434  loss_box_reg: 0.2316  loss_rpn_cls: 0.08883  loss_rpn_loc: 0.197  time: 0.9637  data_time: 0.5305  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:40:19 d2.utils.events]:  eta: 0:00:41  iter: 1059  total_loss: 0.6463  loss_cls: 0.1306  loss_box_reg: 0.2418  loss_rpn_cls: 0.09135  loss_rpn_loc: 0.1796  time: 0.9604  data_time: 0.5216  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:40:41 d2.utils.events]:  eta: 0:00:35  iter: 1079  total_loss: 0.6549  loss_cls: 0.1274  loss_box_reg: 0.2445  loss_rpn_cls: 0.08334  loss_rpn_loc: 0.1783  time: 0.9628  data_time: 0.8087  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:40:57 d2.utils.events]:  eta: 0:00:30  iter: 1099  total_loss: 0.6919  loss_cls: 0.1489  loss_box_reg: 0.2356  loss_rpn_cls: 0.07273  loss_rpn_loc: 0.1469  time: 0.9602  data_time: 0.4169  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:41:18 d2.utils.events]:  eta: 0:00:24  iter: 1119  total_loss: 0.66  loss_cls: 0.1404  loss_box_reg: 0.2531  loss_rpn_cls: 0.0628  loss_rpn_loc: 0.1742  time: 0.9615  data_time: 0.7067  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:41:41 d2.utils.events]:  eta: 0:00:18  iter: 1139  total_loss: 0.735  loss_cls: 0.1588  loss_box_reg: 0.2816  loss_rpn_cls: 0.0643  loss_rpn_loc: 0.1868  time: 0.9650  data_time: 0.7461  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:42:04 d2.utils.events]:  eta: 0:00:12  iter: 1159  total_loss: 0.7333  loss_cls: 0.1395  loss_box_reg: 0.3091  loss_rpn_cls: 0.07494  loss_rpn_loc: 0.1905  time: 0.9678  data_time: 0.8260  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:42:20 d2.utils.events]:  eta: 0:00:06  iter: 1179  total_loss: 0.6085  loss_cls: 0.1283  loss_box_reg: 0.192  loss_rpn_cls: 0.07566  loss_rpn_loc: 0.1521  time: 0.9650  data_time: 0.5238  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:42:42 d2.utils.events]:  eta: 0:00:00  iter: 1199  total_loss: 0.5609  loss_cls: 0.1081  loss_box_reg: 0.2193  loss_rpn_cls: 0.08055  loss_rpn_loc: 0.1423  time: 0.9664  data_time: 0.7568  lr: 0.00025  max_mem: 6923M\n",
      "[11/11 05:42:42 d2.engine.hooks]: Overall training speed: 1198 iterations in 0:19:17 (0.9664 s / it)\n",
      "[11/11 05:42:42 d2.engine.hooks]: Total training time: 0:19:20 (0:00:02 on hooks)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRVEiICco3SV"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VYCIXdMZvDYL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\") # pretrain model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_hRCf86KGi5v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for idx in [18, 16, 37]:\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_train\"), scale=0.5, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D0wRdlcKo6BD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/11 05:45:57 d2.evaluation.coco_evaluation]: Trying to convert 'data_detection_val' to COCO format ...\n",
      "WARNING [11/11 05:45:57 d2.data.datasets.coco]: Using previously cached COCO format annotations at './output/data_detection_val_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "[11/11 05:45:59 d2.data.build]: Distribution of instances among all 5 categories:\n",
      "|  category   | #instances   |  category   | #instances   |  category   | #instances   |\n",
      "|:-----------:|:-------------|:-----------:|:-------------|:-----------:|:-------------|\n",
      "| not plane 1 | 0            | not plane 2 | 0            | not plane 3 | 0            |\n",
      "| not plane 4 | 0            |    plane    | 1596         |             |              |\n",
      "|    total    | 1596         |             |              |             |              |\n",
      "[11/11 05:45:59 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[11/11 05:45:59 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...\n",
      "[11/11 05:45:59 d2.data.common]: Serialized dataset takes 2.21 MiB\n",
      "[11/11 05:45:59 d2.evaluation.evaluator]: Start inference on 40 batches\n",
      "[11/11 05:46:03 d2.evaluation.evaluator]: Inference done 11/40. Dataloading: 0.0307 s/iter. Inference: 0.0455 s/iter. Eval: 0.0003 s/iter. Total: 0.0765 s/iter. ETA=0:00:02\n",
      "[11/11 05:46:08 d2.evaluation.evaluator]: Inference done 29/40. Dataloading: 0.1897 s/iter. Inference: 0.0492 s/iter. Eval: 0.0003 s/iter. Total: 0.2393 s/iter. ETA=0:00:02\n",
      "[11/11 05:46:13 d2.evaluation.evaluator]: Inference done 36/40. Dataloading: 0.2898 s/iter. Inference: 0.0578 s/iter. Eval: 0.0003 s/iter. Total: 0.3481 s/iter. ETA=0:00:01\n",
      "[11/11 05:46:14 d2.evaluation.evaluator]: Total inference time: 0:00:11.831583 (0.338045 s / iter per device, on 1 devices)\n",
      "[11/11 05:46:14 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:02 (0.057637 s / iter per device, on 1 devices)\n",
      "[11/11 05:46:14 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[11/11 05:46:14 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[11/11 05:46:14 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[11/11 05:46:14 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[11/11 05:46:14 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "[11/11 05:46:14 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[11/11 05:46:14 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.309\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.321\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.243\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.601\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.341\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.245\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.742\n",
      "[11/11 05:46:14 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 30.895 | 53.243 | 32.139 | 24.278 | 51.053 | 60.072 |\n",
      "[11/11 05:46:14 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
      "| category    | AP   | category    | AP     | category    | AP   |\n",
      "|:------------|:-----|:------------|:-------|:------------|:-----|\n",
      "| not plane 1 | nan  | not plane 2 | nan    | not plane 3 | nan  |\n",
      "| not plane 4 | nan  | plane       | 30.895 |             |      |\n",
      "OrderedDict([('bbox', {'AP': 30.89511868510649, 'AP50': 53.24317686893616, 'AP75': 32.138898691300014, 'APs': 24.277829017206678, 'APm': 51.05269467906619, 'APl': 60.07202253152929, 'AP-not plane 1': nan, 'AP-not plane 2': nan, 'AP-not plane 3': nan, 'AP-not plane 4': nan, 'AP-plane': 30.89511868510649})])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"data_detection_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwYvbwcjpKBk"
   },
   "source": [
    "### Improvements\n",
    "\n",
    "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xdR6KbCZpOlk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modification done above\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Bring any changes and updates regarding the improvement in here\n",
    "'''\n",
    "print(\"Modification done above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98_M4TooqSs2"
   },
   "source": [
    "## Part 2: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByAEsMtIPLrO"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "peQ95zLuIpkk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
    "# idx is the index of the target bbox in the data\n",
    "# high-resolution image could be passed or could be load from data['file_name']\n",
    "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask\n",
    "#     to convert the segmentation annotations to binary masks\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "from detectron2.utils.visualizer import GenericMask\n",
    "cache_dir = os.path.join(BASE_DIR, \"data\", \"cache-new\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "big_cache = {}\n",
    "queue = []\n",
    "def get_instance_sample(data, idx, img=None):\n",
    "    height, width = data['height'], data['width']\n",
    "    bbox = data['annotations'][idx]['bbox']\n",
    "    x1, y1 = int(bbox[0]), int(bbox[1])\n",
    "    x2, y2 = x1 + int(bbox[2]), y1 + int(bbox[3])\n",
    "    cache_path = os.path.join(cache_dir, os.path.basename(data['file_name']) + f\"-{idx}.png\")\n",
    "    if not os.path.isfile(cache_path):\n",
    "        if data['file_name'] not in big_cache:\n",
    "            if len(big_cache) >= 2:\n",
    "                big_cache[queue.pop()] = None\n",
    "            big_cache[data['file_name']] = cv2.imread(data['file_name'])\n",
    "            queue.append(data['file_name'])\n",
    "        cv2.imwrite(cache_path, big_cache[data['file_name']][y1:y2,x1:x2,:])\n",
    "    obj_img = cv2.imread(cache_path)\n",
    "    obj_mask = np.zeros((int(bbox[3]), int(bbox[2])))\n",
    "    if len(data['annotations'][idx]['segmentation']) > 0 and sum(len(a) for a in data['annotations'][idx]['segmentation']) > 0:\n",
    "        obj_mask = GenericMask(data['annotations'][idx]['segmentation'], height, width).mask[y1:y2,x1:x2]\n",
    "    obj_img = cv2.resize(obj_img, (128, 128))\n",
    "    obj_mask = cv2.resize(obj_mask, (128, 128))\n",
    "    return obj_img, obj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sxrc9X_pjzj-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# We have provided a template data loader for your segmentation training\n",
    "# You need to complete the __getitem__() function before running the code\n",
    "# You may also need to add data augmentation or normalization in here\n",
    "'''\n",
    "\n",
    "class PlaneDataset(Dataset):\n",
    "    def __init__(self, set_name, data_list, flip=False):\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
    "            transforms.Normalize((123.675, 116.28, 103.53), (58.395, 57.12, 57.375)),\n",
    "        ])\n",
    "        self.transforms_flip = transforms.Compose([])\n",
    "        if flip:\n",
    "            self.transforms_flip = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.RandomVerticalFlip(0.5),\n",
    "            ])\n",
    "        self.set_name = set_name\n",
    "        self.data = data_list\n",
    "        self.instance_map = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            for j in range(len(d['annotations'])):\n",
    "                self.instance_map.append([i,j])\n",
    "\n",
    "    '''\n",
    "    # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
    "    # make sure to use the correct length for the final training\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.instance_map)\n",
    "\n",
    "    def numpy_to_tensor(self, img, mask):\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        mask = torch.tensor(mask, dtype=torch.float)\n",
    "        both_images = torch.cat((img, mask.unsqueeze(0)), 0)\n",
    "        both_images = self.transforms_flip(both_images)\n",
    "        img, mask = both_images[0:3], both_images[3]\n",
    "        return img, mask\n",
    "\n",
    "    '''\n",
    "    # Complete this part by using get_instance_sample function\n",
    "    # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
    "    # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
    "    # TODO: 5 lines\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx = self.instance_map[idx]\n",
    "        data = self.data[idx[0]]\n",
    "\n",
    "        img, mask = get_instance_sample(data, idx[1])\n",
    "        img, mask = self.numpy_to_tensor(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "def get_plane_dataset(set_name='train', batch_size=2, flip=False, shuffle=False):\n",
    "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
    "    dataset = PlaneDataset(set_name, my_data_list, flip=flip)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=8, pin_memory=True, shuffle=shuffle)\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6eH3NKaQQfc"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PeqR3s3dSBPN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
    "'''\n",
    "from farseg import farseg_resnet50 as MyModel\n",
    "# Code is too long to put here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj86vD9QT_Z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MaZuO4SKSBuF"
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005372285842895508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 16,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1596,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5563b0c52d480f9818d8b37fc8fe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4, 3, 3, 3], expected input[4, 128, 128, 3] to have 3 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_853/3321007099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_853/601757523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_853/601757523.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4, 3, 3, 3], expected input[4, 128, 128, 3] to have 3 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# The following is a basic training procedure to train the network\n",
    "# You need to update the code to get the best performance\n",
    "# TODO: approx ? lines\n",
    "'''\n",
    "\n",
    "# Set the hyperparameters\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "learning_rate = 0.1\n",
    "weight_decay = 0.0001\n",
    "\n",
    "model = MyModel() # initialize the model\n",
    "# model.load_state_dict(torch.load('{}/output/99_segmentation_model_train_by_split.pth'.format(BASE_DIR)))\n",
    "model = model.cuda() # move the model to GPU\n",
    "loader, _ = get_plane_dataset('train', batch_size, flip=True, shuffle=True) # initialize data_loader\n",
    "crit = nn.BCELoss() # Define the loss function\n",
    "optim = torch.optim.SGD(model.parameters(), momentum=momentum, lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "# scheduler = CosineAnnealingLR(optim, T_max=num_epochs*len(loader), eta_min=1e-6) #learning rate decay\n",
    "max_iters = num_epochs*len(loader)\n",
    "scheduler = MultiStepLR(optim, milestones=[int(max_iters*0.3),int(max_iters*0.6),int(max_iters*0.8)], gamma=0.2) #learning rate decay\n",
    "# start the training procedure\n",
    "for epoch in range(num_epochs):\n",
    "  total_loss = 0\n",
    "  for (img, mask) in tqdm(loader):\n",
    "    img = img.to(device=torch.device('cuda'))\n",
    "    mask = mask.to(device=torch.device('cuda'))\n",
    "    pred = model(img)\n",
    "    loss = crit(pred, mask)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=35, norm_type=2)\n",
    "    optim.step()\n",
    "    total_loss += loss.cpu().data\n",
    "    scheduler.step()\n",
    "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
    "  with open(\"lab3-part2-train.log\", \"a\", encoding=\"utf8\") as f:\n",
    "    f.write(\"Epoch: {}, Loss: {}\\n\".format(epoch, total_loss/len(loader)))\n",
    "  if epoch % 9 == 0:\n",
    "    torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
    "  print(\"Next learning rate:\", scheduler.get_last_lr())\n",
    "\n",
    "'''\n",
    "# Saving the final model\n",
    "'''\n",
    "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dyez1fyQYw7"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDeViryUSCL2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Before starting the evaluation, you need to set the model mode to eval\n",
    "# You may load the trained model again, in case if you want to continue your code later\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "batch_size = 32\n",
    "model = MyModel().cuda()\n",
    "model.load_state_dict(torch.load('{}/output/999_segmentation_model_resnet_train_by_split.pth'.format(BASE_DIR)))\n",
    "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
    "loader, dataset = get_plane_dataset('val', batch_size, flip=False)\n",
    "\n",
    "total_iou = 0\n",
    "images = 0\n",
    "SMOOTH = 1e-6\n",
    "for (img, mask) in tqdm(loader):\n",
    "  with torch.no_grad():\n",
    "    img = img.cuda()\n",
    "    mask = mask.cuda()\n",
    "    pred = model(img)\n",
    "    pred = pred > 0\n",
    "    mask = mask > 0.5\n",
    "    \n",
    "    intersection = (pred & mask).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (pred | mask).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    thresholded = iou\n",
    "    total_iou += sum(thresholded.tolist())\n",
    "    images += len(thresholded.tolist())\n",
    "\n",
    "    '''\n",
    "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
    "    '''\n",
    "\n",
    "\n",
    "print(\"\\n #images: {}, Mean IoU: {}\".format(images, total_iou/images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbmTj9JICiKz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize 3 sample outputs\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "for (img, mask) in loader:\n",
    "    break\n",
    "with torch.no_grad():\n",
    "  img = img.cuda()\n",
    "  mask = mask.cuda()\n",
    "  pred = model(img)\n",
    "  pred_mask = torch.zeros_like(pred, device=pred.device)\n",
    "  pred_mask[pred > 0.5] = 255\n",
    "  pred *= 255\n",
    "  mask *= 255\n",
    "  mask = mask.cpu().numpy()\n",
    "  pred = pred.cpu().numpy()\n",
    "  pred_mask = pred_mask.cpu().numpy()\n",
    "  img = img.cpu().numpy()\n",
    "  for k in range(batch_size):\n",
    "    p, pm, m, i = pred[k, ...], pred_mask[k, ...], mask[k, ...], img[k, ...]\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    ax = fig.subplots(nrows=1, ncols=4)\n",
    "    ax[0].imshow(p, cmap='gray')\n",
    "    ax[1].imshow(pm, cmap='gray')\n",
    "    ax[2].imshow(m, cmap='gray')\n",
    "    std = [58.395, 57.12, 57.375]\n",
    "    mean = [123.675, 116.28, 103.53]\n",
    "    for d in range(3):\n",
    "      i[d, ...] = i[d, ...] * std[d] + mean[d]\n",
    "    i = torch.tensor(i).permute((1,2,0)).numpy()\n",
    "    ax[3].imshow(i)\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, f\"val_set_{k+1}.png\"))\n",
    "    plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "navoiGdrqaZT"
   },
   "source": [
    "## Part 3: Instance Segmentation\n",
    "\n",
    "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwk33DGBowP"
   },
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrZ8TG-Ot2J"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define a new function to obtain the prediction mask by passing a sample data\n",
    "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
    "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
    "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "\n",
    "def get_prediction_mask(data):\n",
    "\n",
    "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc7TSK6EBi9u"
   },
   "source": [
    "### Visualization and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7T2YX8MBiGO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPo_03up-g_f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
    "'''\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
    "    if(len(dots)==0):\n",
    "      return []\n",
    "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
    "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
    "    tmpdots = dots[inds]\n",
    "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
    "    inds = inds[1:] - inds[:-1]\n",
    "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
    "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
    "    return ' '.join([str(i) for i in runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv0rab2LJev-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# You need to upload the csv file on kaggle\n",
    "# The speed of your code in the previous parts highly affects the running time of this part\n",
    "'''\n",
    "\n",
    "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the training set\n",
    "'''\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for index in inds:\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index)\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "'''\n",
    "# Writing the predictions of the test set\n",
    "'''\n",
    "\n",
    "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
    "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
    "  sample = my_data_list[i]\n",
    "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
    "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
    "  inds = torch.unique(pred_mask)\n",
    "  if(len(inds)==1):\n",
    "    preddic['ImageId'].append(sample['image_id'])\n",
    "    preddic['EncodedPixels'].append([])\n",
    "  else:\n",
    "    for j, index in enumerate(inds):\n",
    "      if(index == 0):\n",
    "        continue\n",
    "      tmp_mask = (pred_mask==index).double()\n",
    "      encPix = rle_encoding(tmp_mask)\n",
    "      preddic['ImageId'].append(sample['image_id'])\n",
    "      preddic['EncodedPixels'].append(encPix)\n",
    "\n",
    "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
    "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
    "pred_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7nN4SduqrpI"
   },
   "source": [
    "## Part 4: Mask R-CNN\n",
    "\n",
    "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axWf7drKNXYd"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC1FDCcQN2LH"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This function should return a list of data samples in which each sample is a dictionary.\n",
    "# Make sure to select the correct bbox_mode for the data\n",
    "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
    "# Other values could be obtained from the image files.\n",
    "# TODO: approx 35 lines\n",
    "'''\n",
    "VAL_RATE = 0.2 # Precentage of the validate size\n",
    "def get_detection_data(set_name, datapath=\"data/train.json\"):\n",
    "    data_dirs = '{}/data'.format(BASE_DIR)\n",
    "    # return test_set, no annotations\n",
    "    if set_name == \"test\":\n",
    "        test_set = []\n",
    "        for fname in os.listdir(os.path.join(data_dirs, \"test\")):\n",
    "            if os.path.splitext(fname)[1] == \".png\":\n",
    "                path = os.path.join(data_dirs, \"test\", fname)\n",
    "                width, height = Image.open(path).size\n",
    "                test_set.append({\n",
    "                    \"file_name\": path,\n",
    "                    \"image_id\": os.path.splitext(fname)[0],\n",
    "                    \"height\": height,\n",
    "                    \"width\": width,\n",
    "                    \"annotations\": []\n",
    "                })\n",
    "        return test_set\n",
    "    # return validate_set or train_set, with annotations\n",
    "    with open(datapath) as f:\n",
    "        data = json.load(f)\n",
    "    validate_size = int(len(data)*VAL_RATE)\n",
    "    train_annotations, validate_annotations = data[0:len(data)-validate_size], data[len(data)-validate_size:]\n",
    "    annotations = validate_annotations if set_name == \"val\" else (data if set_name == \"all\" else train_annotations)\n",
    "    # return validate_set or train_set, with annotations\n",
    "    datadict = {}\n",
    "    for annotation in annotations:\n",
    "        path = os.path.join(data_dirs, \"train\", annotation[\"file_name\"])\n",
    "        anno = {\n",
    "            \"bbox\": annotation[\"bbox\"],\n",
    "            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "            \"segmentation\": annotation[\"segmentation\"],\n",
    "            \"category_id\": annotation[\"category_id\"],\n",
    "            \"iscrowd\": annotation[\"iscrowd\"],\n",
    "            \"area\": annotation[\"area\"]\n",
    "        }\n",
    "        if path in datadict:\n",
    "            datadict[path][\"annotations\"].append(anno)\n",
    "            continue\n",
    "        width, height = Image.open(path).size\n",
    "        datadict[path] = {\n",
    "            \"image_id\": annotation[\"image_id\"],\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"annotations\": [{\n",
    "                \"bbox\": annotation[\"bbox\"],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": annotation[\"segmentation\"],\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"iscrowd\": annotation[\"iscrowd\"],\n",
    "                \"area\": annotation[\"area\"]\n",
    "            }]\n",
    "        }\n",
    "    return [{\"file_name\": path, **data} for path, data in datadict.items()]\n",
    "\n",
    "'''\n",
    "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
    "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
    "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "for i in [\"train\", \"val\", \"all\", \"test\"]:\n",
    "    DatasetCatalog.register(\"data_detection_{}\".format(i), lambda i=i: get_detection_data(i))\n",
    "    MetadataCatalog.get(\"data_detection_{}\".format(i)).set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])\n",
    "\n",
    "DatasetCatalog.register(\"data_detection_all_ori\", lambda i=i: get_detection_data(\"all\", datapath=\"data/train.json\"))\n",
    "MetadataCatalog.get(\"data_detection_all_ori\").set(thing_classes=[\"not plane 1\", \"not plane 2\", \"not plane 3\", \"not plane 4\", \"plane\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG5slAhQNjE7"
   },
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG1pnKLMOcjS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Set the configs for the detection part in here.\n",
    "# TODO: approx 15 lines\n",
    "'''\n",
    "cfg = get_cfg()\n",
    "# model settings\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "# training settings\n",
    "cfg.DATASETS.TRAIN = (\"data_detection_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") # pretrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ifeV1sNvtt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4K0Nz5OeKk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Create a DefaultTrainer using the above config and train the model\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "298QruFnNxyn"
   },
   "source": [
    "### Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcGwV5-9Oetp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
    "# Define a DefaultPredictor\n",
    "'''\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_3wS2BFLLGp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Visualize the output for 3 random test samples\n",
    "# TODO: approx 10 lines\n",
    "'''\n",
    "test_set = get_detection_data(\"test\")\n",
    "for i in range(3):\n",
    "    idx = random.randrange(0, len(test_set))\n",
    "    data = test_set[idx]\n",
    "    img = cv2.imread(data[\"file_name\"])\n",
    "    result = predictor(img)\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"data_detection_test\"), scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
    "    result = visualizer.draw_instance_predictions(result[\"instances\"].to(\"cpu\"))\n",
    "    img = result.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(os.path.join(cfg.OUTPUT_DIR, f\"test_set_{idx}.png\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Use COCOEvaluator and build_detection_train_loader\n",
    "# You can save the output predictions using inference_on_dataset\n",
    "# TODO: approx 5 lines\n",
    "'''\n",
    "evaluator = COCOEvaluator(\"data_detection_val\", tasks=(\"segm\",), output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
    "print(inference_on_dataset(predictor.model, test_loader, evaluator))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
